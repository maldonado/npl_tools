\vspace{3mm}
\noindent\rqi
\vspace{3mm}

\noindent \textbf{Motivation:} As shown in previous work \cite{Potdar2014ICSME, Maldonado2015MTD}, \SATD comments can be found in the source code. However, there is not an optimal way to identify these technical debt comments yet. The methods proposed so far heavily relies on manual examination, and there are too little evidence on how well these approaches perform and how applicable they are. Answering this question is important to help us understand the opportunities and limitations of NLP techniques in identifying \SATD comments. 

\vspace{1mm}
\noindent \textbf{Approach:} As described in Section \ref{sec:approach}, we manually classify comments from ten open source projects into one of the following types of \SATD: design debt, defect debt, implementation debt, documentation debt and test debt. However, as shown in our previous work \cite{Maldonado2015MTD}, the most frequent \SATD comments are classified as design debt, implementation debt or defect debt respectively. Therefore, in our case study we focuses our attention in these three \SATD types. 

We design our case study to test how well the a maximum entropy classifier can perform when trained with our dataset. In order to do that, we first prepare the training data set. The training dataset is composed by two different columns, one is the classification and the other one is the source code comment. Then, we select all comments that were classified as not containing \SATD and the comments classified as the specific type of \SATD  that we want to predict. these comments from 9 out of 10 projects that we analyzed. Second, for the test dataset, we use the comments in the one project that was out of the training dataset and run the analysis.  

For example, if we want to classify the \SATD comments in Apache Ant project we create a training dataset using all comments from the other nine projects, namely Apache Jmeter, ArgoUML, Columba, EMF, Hibernate, JEdit, JFreeChart, JRuby and SQuirrel. Then, we use the comments of Apache Ant project to create the test dataset. 

\vspace{1mm}
\noindent \textbf{Results:} Following this approach we executed the classifier for all projects, collecting results for three different types of \SATD: design debt, implementation debt and defect debt. Tables \ref{tbl:classifier_results_design}, \ref{tbl:classifier_results_implementation} and \ref{tbl:classifier_results_defect} presents precision, recall, F1 measure, random precision, random recall and random F1 measure for each project. Taking JRuby as an example while classifying design debt in Table \ref{tbl:classifier_results_design}, we find that the classifier reached a precision of 82\% and recall of 76\% resulting in a F1 measure of 79\%. For the same project, the calculated random values for precision, recall and F1 measure are 7\%, 50\% and 13\% respectively. Comparing both F1 measures, the result obtained with the classifier is over than 6 times higher than the random one.

Regarding the results for design debt in Table \ref{tbl:classifier_results_design}, ArgoUml \SATD comments were classified with precision of 76.9\% and recall of 87.9\%, and was the higher F1 measure (i.e., 81.9\%) within our selected projects. On the contrary, EMF was classified with a precision of 52.1\%, recall of 32.1\% resulting in a F1 measure of 39.7\%. The average F1 measure considering  all projects is of 59.75\%, whereas the random F1 measure is of 8.61\%. 

For requirement debt results shown in Table \ref{tbl:classifier_results_implementation} we find that the F1 measure of the classification ranges from 11.8\% to 93.5\% for EMF and Columba. Although 11.8\% is not a very hight value the random F1 measure for EMF is less than 0\%. In average the F1 measure of all projects is of 52.28\%, and the random F1 measure is of 3.2\%. 

Lastly, for the defect debt classification results shown in Table \ref{tbl:classifier_results_defect} we achieved a F1 measure average of 25.32\% while the random F1 measure is of 1.6\%/. 
Although the F1 measure average still higher than the random F1 measure average, defect debt classification has the lowest performance. We argue that the low number of \SATD comments used in the training dataset plays a major role in this result. 

For all the types above, the random values are particularly low due to the fact that the number of comments that does not represent any kind of \SATD is much higher than the ones which represent. The number of \SATD comments are presented in Table \ref{tbl:td_distribution}. We show in this table the exact number of \SATD comments found in each project separated by the specific technical debt type: design debt, requirement debt, defect debt. Documentation debt and test debt are shown under the column `Other' as they number of occurrences were not as frequent as of the other types. 

\begin{table}[!hbt]
    \begin{center}
        \caption{Improvement over the random F1 measure for design}
        \label{tbl:improvement_f1measure_design}
        \begin{tabular}{l| c c c }
        \toprule
        \textbf{Project} & \textbf{F1} & \thead{Rnd\\F1} & \textbf{Improvement}\\
        \midrule
         Apache Ant      &  0.471      &  0.045          & 10x\\
         Apache Jmeter   &  0.704      &  0.075          & 9x\\
         ArgoUML         &  0.819      &  0.151          & 5x\\
         Columba         &  0.586      &  0.038          & 15x\\
         EMF             &  0.397      &  0.035          & 11x\\
         Hibernate       &  0.692      &  0.214          & 3x\\
         JEdit           &  0.465      &  0.037          & 12x\\
         JFreeChart      &  0.503      &  0.080          & 6x\\
         JRuby           &  0.795      &  0.131          & 6x\\
         SQuirrel        &  0.543      &  0.055          & 9x\\
        \bottomrule
        \end{tabular}
    \end{center}    
\end{table}

\begin{table}[!hbt]
    \begin{center}
        \caption{Improvement over the random F1 measure for requirements}
        \label{tbl:improvement_f1measure_requirement}
        \begin{tabular}{l| c c c }
        \toprule
        \textbf{Project} & \textbf{F1} & \thead{Rnd\\F1} & \textbf{Improvement}\\
        \midrule
         Apache Ant      & 0.462       &  0.008          & 57x\\
         Apache Jmeter   & 0.393       &  0.005          & 78x\\
         ArgoUML         & 0.741       &  0.125          & 5x \\
         Columba         & 0.935       &   0.04          & 23x\\
         EMF             & 0.118       &  0.007          & 16x\\
         Hibernate       & 0.650       &  0.042          & 15x\\
         JEdit           & 0.100       &  0.003          & 33x\\
         JFreeChart      & 0.513       &  0.011          & 46x\\
         JRuby           & 0.463       &  0.045          & 10x\\
         SQuirrel        & 0.853       &   0.04          & 21x\\
        \bottomrule
        \end{tabular}
    \end{center}    
\end{table}


\conclusionbox{We find that NLP techniques, such as maximum entropy classifiers, can be used effectively to find \SATD comments. We achieved an average F1 measure of \todo{} for design debt, an average of \todo{} while classifying requirement debt, and for defect debt our average was of \todo{}. For all three types that we classify using our dataset we perform better than the random F1 measure average, moreover our classified F1 measure is \todo{x} times better than the random F1 measure.}

\vspace{3mm}
\noindent\rqii
\vspace{3mm}

\noindent \textbf{Motivation:} After asserting the efficiency of NPL classifiers predicting \SATD we want to better understand which are the comment patterns or the most common features that developers use when writing \SATD comments. Answering this question will provide insightful information that can guide future research direction and broaden our understanding on \SATD.     

\vspace{1mm}
\noindent \textbf{Approach:} In order to 
explicar o que eh feature
falar sobre como o peso eh atribuido a cada feature , citar o eh atraves da obsevacao empirica ... se nao me engano
explicar como o voto para uma categoria eh feito
falar que coletamos as features que tem um peso maior peso

\vspace{1mm}
\noindent \textbf{Results:} 
coletar as tops features de todos os projetos
mostrar as features para design 
mostrar as features para requirement
comentar sobre as features, provavelmente vai existir overlapping

concluir com as features mais comuns.
\conclusionbox{}

\vspace{3mm}
\noindent\rqiii
\vspace{3mm}

\noindent \textbf{Motivation:} 

\vspace{1mm}
\noindent \textbf{Approach:} 

\vspace{1mm}
\noindent \textbf{Results:} 

\conclusionbox{}