\vspace{3mm}
\noindent\rqi
\vspace{3mm}

\noindent \textbf{Motivation:} As shown in previous work \cite{Potdar2014ICSME, Maldonado2015MTD}, \SATD comments can be found in the source code. However, there is not an optimal way to identify these technical debt comments yet. The methods proposed so far heavily relies on manual examination, and there are too little evidence on how well these approaches perform and how applicable they are. Answering this question is important to help us understand the opportunities and limitations of NLP techniques in identifying \SATD comments. 

\vspace{1mm}
\noindent \textbf{Approach:} As described in Section \ref{sec:approach}, we manually classify comments from ten open source projects into one of the following types of \SATD: design debt, defect debt, implementation debt, documentation debt and test debt. However, as shown in our previous work \cite{Maldonado2015MTD}, the most frequent \SATD comments are classified as design debt and implementation debt respectively. Therefore, in our case study we focuses our attention in these two \SATD types. 

We design our case study to test how well the maximum entropy classifier can predict \SATD comments when trained with our dataset. In order to do that, we must first prepare the training data set. The training dataset is composed by two different columns, one column is the classification and the other one is the source code comment. Then, we select all comments that were classified as not containing \SATD and the comments classified as the specific type of \SATD  that we want to predict from 9 out of 10 projects. Second, to assert the performance of the prediction we create a test dataset. The test dataset is composed by all the comments which does not contains \SATD and the specific technical debt that we are predicting from the one project that was not used in the training dataset. 

That way, if we want to classify the \SATD comments in Apache Ant project we create a training dataset using all comments from the other nine projects, namely Apache Jmeter, ArgoUML, Columba, EMF, Hibernate, JEdit, JFreeChart, JRuby and SQuirrel. Then, we use the comments of Apache Ant project to create the test dataset. Similarly with the training dataset, the test dataset possesses two columns one for the classification and other for the comment. 

Third, the classifier is executed and based on the training dataset it gives a classification for each one of the comments in the test dataset. The tool compares the classification provided in the test dataset with the predicted classification given during the execution. The results of this computation are provided in terms of precision, recall and the F1 measure achieved by the classifier.

In order to determine if the classification of the NLP tool are being effective or not we compare the obtained results with the results of two other baselines. The first one is the state-of-the-art approach in detection of \SATD comments devised by Potdar and Shihab ~\cite{Potdar2014ICSME}. This approach uses 62 comment patterns (i.e., key words) that were noticed as recurrent in \SATD comments while doing a manual inspection of 101,762 comments. The second approach used is weighed random. In fact, \SATD comments are not as frequent as comments that does not have \SATD. Through weighed random precision, recall and F1 measure is possible to show this discrepancy in the data distribution, and show how well the classifier performs compared with a random prediction.

Lastly, for each project we calculated both baselines and executed the classification process storing the results in the database to perform our analysis.

\vspace{1mm}

\noindent \textbf{Results:} We find that the predicted classification of the NLP tool greatly outperforms the current state-of-the-art approach to identify \SATD baseline and the weighed random baseline as well. Figure \ref{fig:f1_measure_comparison_design_debt} presents the comparison between the predicted F1 measure, the state-of-the-art baseline F1 measure and the weighed random F1 measure obtained during the classification of design debt. We choose to use the F1 measure to compare the approach as this measurement takes into consideration the combination of precision and recall.

It is important to notice that each one of the selected baselines for comparison has one strong point. The comment patterns approach has a high precision, but it lacks recall, i.e., this approach points correctly to \SATD comments, but it identifies a very small set of all the \SATD comments in the project. The less sophisticated weighed random baseline provides a high recall because of the two categories that we try to predicted i.e, therefore 50\% chance to randomly get one of the two classifications. 

\begin{figure}[thb!]
   \centering
  \includegraphics[width=0.48\textwidth]{figures/f1_measure_comparisom_design_2.pdf}
  \vspace{-3mm}
  \caption{F1 measure comparison Design Debt}
  \label{fig:f1_measure_comparison_design_debt}
\end{figure}

For all the projects, the F1 measure achieved by the NLP classifier is higher than the the others F1 measures. The highest F1 measure obtained for design debt was in ArgoUML with a value of 0.814. In the other hand, the lowest F1 measure obtained was in EMF with a value of 0.470. In average the F1 measure obtained was of 0.620. In comparison, the average F1 measure obtained using the state-of-the-art approach was of 0.099, and the weighed random F1 measure 0.0861. Consequently, our approach to identify design debt represent, in average, a improvement of 6 times the state of the art approach and 7 times the weighed random value. In the  Table \ref{tbl:improvement_f1measure_design} shows the improvement of our approach over the other the comment pattern baseline for each project. 
 
Similarly, Figure \ref{fig:f1_measure_comparison_requeriment} shows the comparison between the three F1 measures while identifying requirement \SATD.

\begin{figure}[thb!]
  \centering
  \includegraphics[width=0.48\textwidth]{figures/f1_measure_comparison_requirement_2.pdf}
  \vspace{-3mm}
  \caption{F1 measure comparison Requirement Debt}
  \label{fig:f1_measure_comparison_requeriment}
\end{figure}

We find that in 90\% of the analyzed projects the F1 measure obtained by our approach surpass the comment pattern baseline, and in 100\% of the cases we outperform the weighed random baseline. In addition to that, the comment patterns baseline calculated is the same for design debt and for requirement debt, since this approach does not distinguish between different types of \SATD as ours. 

The highest F1 measure achieved by our approach was on Apache Ant with 0.987, and the lowest value was off 0.091. Although there was a big fluctuation between the maximum and minimum value obtained our average F1 measure was of 0.572. Despite the fact that this number is lower than the average obtained for the identification of design debt, this average still impressive due the reduced amount of requirement \SATD comments that are in the projects. For example, Apache Ant has only 16 occurrences of requirement \SATD scattered among 4,045 comments.  

Table \ref{tbl:improvement_f1measure_requirement} shows

 % Tables \ref{tbl:classifier_results_design}, \ref{tbl:classifier_results_implementation} and \ref{tbl:classifier_results_defect} presents precision, recall, F1 measure, random precision, random recall and random F1 measure for each project. Taking JRuby as an example while classifying design debt in Table \ref{tbl:classifier_results_design}, we find that the classifier reached a precision of 82\% and recall of 76\% resulting in a F1 measure of 79\%. For the same project, the calculated random values for precision, recall and F1 measure are 7\%, 50\% and 13\% respectively. Comparing both F1 measures, the result obtained with the classifier is over than 6 times higher than the random one.

% Regarding the results for design debt in Table \ref{tbl:classifier_results_design}, ArgoUml \SATD comments were classified with precision of 76.9\% and recall of 87.9\%, and was the higher F1 measure (i.e., 81.9\%) within our selected projects. On the contrary, EMF was classified with a precision of 52.1\%, recall of 32.1\% resulting in a F1 measure of 39.7\%. The average F1 measure considering  all projects is of 59.75\%, whereas the random F1 measure is of 8.61\%. 

% For requirement debt results shown in Table \ref{tbl:classifier_results_implementation} we find that the F1 measure of the classification ranges from 11.8\% to 93.5\% for EMF and Columba. Although 11.8\% is not a very hight value the random F1 measure for EMF is less than 0\%. In average the F1 measure of all projects is of 52.28\%, and the random F1 measure is of 3.2\%. 

Although the F1 measure average still higher than the random F1 measure average, defect debt classification has the lowest performance. We argue that the low number of \SATD comments used in the training dataset plays a major role in this result. 

For all the types above, the random values are particularly low due to the fact that the number of comments that does not represent any kind of \SATD is much higher than the ones which represent. The number of \SATD comments are presented in Table \ref{tbl:td_distribution}. We show in this table the exact number of \SATD comments found in each project separated by the specific technical debt type: design debt, requirement debt, defect debt. Documentation debt and test debt are shown under the column `Other' as they number of occurrences were not as frequent as of the other types. 

\begin{table}[!hbt]
    \begin{center}
        \caption{Improvement over the random F1 measure for design}
        \label{tbl:improvement_f1measure_design}
        \begin{tabular}{l| c c c }
        \toprule
        \textbf{Project} & \textbf{F1} & \thead{Baseline\\F1} & \textbf{Improvement}\\
        \midrule
         Apache Ant      &  0.517      &  0.142          &  3.6  x\\
         Apache Jmeter   &  0.731      &  0.108          &  6.7  x\\
         ArgoUML         &  0.814      &  0.040          &  20.3 x\\
         Columba         &  0.601      &  0.065          &  9.2  x\\
         EMF             &  0.470      &  0.087          &  5.4  x\\
         Hibernate       &  0.744      &  0.118          &  6.3  x\\
         JEdit           &  0.509      &  0.264          &  1.9  x\\
         JFreeChart      &  0.492      &  0.044          &  11.1 x\\
         JRuby           &  0.783      &  0.078          &  10   x\\
         SQuirrel        &  0.540      &  0.044          &  12.2 x\\
        \bottomrule
        \end{tabular}
    \end{center}    
\end{table}

\begin{table}[!hbt]
    \begin{center}
        \caption{Improvement over the random F1 measure for requirements}
        \label{tbl:improvement_f1measure_requirement}
        \begin{tabular}{l| c c c }
        \toprule
        \textbf{Project} & \textbf{F1} & \thead{Rnd\\F1} & \textbf{Improvement}\\
        \midrule
         Apache Ant      & 0.987       & 0.142          &  6.9  x\\
         Apache Jmeter   & 0.255       & 0.108          &  2.3  x\\
         ArgoUML         & 0.760       & 0.040          &  19   x\\
         Columba         & 0.934       & 0.065          &  14.3 x\\
         EMF             & 0.381       & 0.087          &  4.3  x\\
         Hibernate       & 0.476       & 0.118          &  4    x\\
         JEdit           & 0.091       & 0.264          &  2.9  x\\
         JFreeChart      & 0.500       & 0.044          &  11.3 x\\
         JRuby           & 0.462       & 0.078          &  5.9  x\\
         SQuirrel        & 0.875       & 0.044          &  19.8 x\\
        \bottomrule
        \end{tabular}
    \end{center}    
\end{table}


\conclusionbox{We find that NLP techniques, such as maximum entropy classifiers, can be used effectively to find \SATD comments. We achieved an average F1 measure of \todo{} for design debt and an average of \todo{} while classifying requirement debt. For the two types that we classify using our dataset we perform better than the random F1 measure average, moreover our classified F1 measure is \todo{x} times better than the random F1 measure.}

\vspace{3mm}
\noindent\rqii
\vspace{3mm}

\noindent \textbf{Motivation:} After asserting the efficiency of NPL classifiers predicting \SATD we want to better understand which are the comment patterns that developers use when writing \SATD comments. Answering this question will provide insightful information that can guide future research direction and broaden our understanding on \SATD.     

\vspace{1mm}
\noindent \textbf{Approach:} As mentioned before, we use the Stanford Classifier to predict \SATD comments, and the first part of the prediction process is to generate the features that will be used to classify the comments. Features, in our case, are fragments of data that possesses a class (i.e., design debt, requirement debt or without technical debt) and a weight. These features are extracted from the comments in the training dataset, and then applied to the test dataset where they are combined to reach a vote. That is, every feature that is satisfied by the comment being classified (i.e., matched) will be used to predict the class for the comment. The vote is given to the class that has more weight, therefore positive weight features that matches the comment being classified will result in a higher weight, and the class that has the higher weight will be assumed as the predicted class.

% maybe explain more details of how the weight is given to each feature

For example, given three different features: `hack' with a weight of 5.3 , `dirty' with weigh of 3.2 both for design debt and `ignore' meaning that the comment is not a technical debt with a weight of 4.1. When classifying the following comment ``this is a dirty hack and should not be ignored'', all features would have been matched, and the vote decision would be like this: design debt weight = 8.5 (i.e., feature 1 plus feature 2) and without classification weight = 4.1 resulting in a comment classified as  design debt.

\vspace{1mm}
\noindent \textbf{Results:} Table \ref{tbl:top_ten_features} shows the top 10 features identifying \SATD in the ten studied projects ordered by weight. The first column we list the features with higher weights classifying  

\begin{table}[!hbt]
    \begin{center}
        \caption{Top ten features for Self-Admitted Technical Debt}
        \label{tbl:top_ten_features}
        \begin{tabular}{l| l c | l c }
        \toprule
        \textbf{Order} & \thead{Design\\TD} & \thead{Requirement\\TD}  \\
        \midrule
         1  & hack       &   todo              \\
         2  & workaround &   needed            \\
         3  & kludge     &   implementation    \\
         4  & yuck!      &   fixme             \\
         5  & fixme      &   xxx               \\
         6  & todo       &   auto-generated    \\
         7  & stupidity  &   ends?             \\
         8  & ugly       &   configurable      \\
         9  & unused?    &   convention        \\
         10 & sucks      &   apparently        \\
        \bottomrule
        \end{tabular}
    \end{center}    
\end{table}

It is possible that the same feature is used to classify design debt and requirement debt. However, the weight of this feature can be different accordingly with the type being classified. As we can see in Table \ref{tbl:top_ten_features}, `FIXME:' appears in both columns although it has a higher weight when classifying requirement debt than design debt. 

In spite of the fact that features with higher weights will have more impact during the classification the final vote is given based on the combined weight of all matching features. In average the number of features to classify design debt was of 6,196 whereas for requirement debt was of 2,889.

\conclusionbox{We find that the most common comment patterns for self-admitted design debt are: `hack', `workaround', `yuck!', `kludge', `stupidity', `needed?', `unused?', `columns??', `FIXME:' and `wtf?'. Whereas, for self-admitted requirement debt, the patterns are: `TODO:', `FIXME:', `needed', `implementation', `ends?', `apparently', `XXX', `configurable', `Auto-generated' and `empty'.}

\vspace{3mm}
\noindent\rqiii
\vspace{3mm}

\noindent \textbf{Motivation:} Answering this research question will allow us to better estimate effort for future research in the area. More specifically, it will be beneficial to 1) make this kind of analysis more efficient, so manual classification of comments, that is time consuming and difficult by nature, can be kept to a minimum. 2) Helping researchers interested in applying the same technique to analyze \SATD in domains not covered in our study, another programming languages than Java, or even comments written in languages other than English. 

\vspace{1mm}
\noindent \textbf{Approach:} We executed the classification process several times with an increasing training dataset while collecting the results to analyze the changes between each iteration. 

We first select a project that will be used to create the test dataset and we keep it separated from the other nine projects. Second, we take the comments of one of the nine remainder projects to create the training dataset. Third, we run the classification process and collect the results. Fourth, we incrementally add the comments of another project to the training dataset and repeat the classification process collecting the results. We cycle trough this steps until we had added all nine projects comments to the training dataset. 

We apply this approach for each one of our ten studied projects classifying design and requirement \SATD comments. We ranked the projects that contained more \SATD of the specific type being classified so that they can be added first to training dataset. 

Intuitively we expect that the performance of the classifier would improve with each iteration as more comments are being added to the dataset. However, we find that this is not what always happen, and that the addition of more comments can decrease the performance of the classifier in given scenarios.

Due to that, we determined which iteration has the highest performance for each project. We called this iteration as the maximum F1 measure. To know how close the other iterations were from reaching the maximum performance we use the iteration F1 measure value to calculate the percentage of the maximum F1 measure it represents.

Lastly, for each type of technical debt classified we determined which is the iteration that is the most significant across all projects by calculating the average of the percentage of the maximum F1 measure.

\vspace{1mm}
\noindent \textbf{Results:} We find that the F1 measure improves significantly in the firsts iterations and, after that, the improvement rate keep a more stead pace. Figures \ref{fig:design_jfreechart_result}, \ref{fig:design_jruby_result} shows this behavior for design \SATD. Similarly, Figure \ref{fig:implementation_argo_result} shows the evolution of the F1 measure over the iterations for requirement \SATD in ArgoUml. 

\begin{figure}[thb!]
   \centering
  \includegraphics[width=0.48\textwidth]{figures/design_jfreechart.pdf}
  \vspace{-3mm}
  \caption{JFreeChart Design Debt classification}
  \label{fig:design_jfreechart_result}
\end{figure}
 
\begin{figure}[thb!]
  \centering
  \includegraphics[width=0.48\textwidth]{figures/design_jruby.pdf}
  \vspace{-3mm}
  \caption{JRuby Design Debt classification}
  \label{fig:design_jruby_result}
\end{figure}

\begin{figure}[thb!]
  \centering
  \includegraphics[width=0.48\textwidth]{figures/implementation_argo.pdf}
  \vspace{-3mm}
  \caption{ArgoUml Requirement Debt classification}
  \label{fig:implementation_argo_result}
\end{figure}

JFreeChart has F1 measure of \todo{} during the first iteration, \todo{} in the third iteration and \todo{} in the ninth iteration which was the highest F1 measure achieved. The improvement in the F1 measure between the first and third iteration is greater than the improvement between the third and ninth iteration. In the third iteration the classifier was trained with \todo{} of the comments that where used in the ninth iteration. A reduction of \todo{} of the necessary training data to achieve almost the same result in terms of F1 measure. 

In the same way, JRuby has a F1 measure value of \todo{} in the third iteration and \todo{} in the ninth iteration which is the maximum F1 measure achieved for the project. The F1 measure achieved in the third iteration represents \todo{} of the maximum F1 measure achieved in the ninth iteration, and this percentage was reached using \todo{} comments whereas the maximum F1 measure used \todo{} comments in the training dataset. Therefore, for JRuby we could achieve \todo{} of the maximum result using only 62\% of the comments.

Based on the average of the percentage of the maximum F1 measure we find that the best performance of the classifier is achieved during the eighth iteration for design \SATD. At this iteration the average of the maximum F1 measure is of 96.57\% using an average of 2,353 comments in the training dataset. In comparison the ninth iteration have an average of the maximum F1 measure of 95.99\% which is slightly lower than the average obtained in the eighth iteration, and uses more comments in the training dataset (i.e., 2,432). 

Table \ref{tbl:design_iteration_performance} presents the percentage of the maximum F1 measure for each one the iterations. The second column shows the average of comments used in the training dataset of that specific iteration. The third column give us the percentage of the total comments that are being used in the training dataset. 

As we can see in Table \ref{tbl:design_iteration_performance}, the third iteration presents an average of the maximum F1 measure of 92.47\%, and uses 1,444 comments in average in the training dataset. This means using only 59.37\% of the total comments classified in this study (i.e., 2,432 design \SATD). 

We also analyzed the average percentage of the maximum F1 measure for requirement \SATD. The seventh iteration was the one with highest average percentage of the maximum F1 measure achieving a value of 89.89\%. In the seventh iteration an average of 1,055 requirement \SATD comments were used in the training dataset represent 97\% of all requirement \SATD comments that we classified during this study.

We also find that the first iteration has a hight average of percentage of the maximum F1 across all projects (i.e., 83\%). Although this percentage still lower than the achieved at the seventh iteration (i.e., 88\%), the training dataset used an average of 600 \SATD comments representing a reduction of 44\% in the necessary comments to create an effective training dataset to classify requirement \SATD comments. Table \ref{tbl:requirement_iteration_performance} shows the average maximum F1 measure for each iteration. 

\begin{table}[!hbt]
    \begin{center}
        \caption{Average maximum F1 measure for Design Debt}
        \label{tbl:design_iteration_performance}
        \begin{tabular}{l| c c }
        \toprule
        \thead{Iteration\\Number} & \thead{\% of maximum\\F1 measure} & \thead{Average\\comments} \\
        \midrule
         1  &  0.684  & 756   \\  
         2  &  0.805  & 1,106 \\  
         3  &  0.863  & 1,444 \\  
         4  &  0.879  & 1,717 \\  
         5  &  0.857  & 1,919 \\  
         6  &  0.931  & 2,108 \\  
         7  &  0.950  & 2,251 \\  
         8  &  0.958  & 2,353 \\  
         9  &  0.966  & 2,432 \\  
        \bottomrule
        \end{tabular}
    \end{center}    
\end{table}

\begin{table}[!hbt]
	\begin{center}
		\caption{Average maximum F1 measure for Requirement Debt}
		\label{tbl:requirement_iteration_performance}
		\begin{tabular}{l| c c }
			\toprule
			\thead{Iteration\\Number} & \thead{\% of maximum\\F1 measure} & \thead{Average\\comments} \\
			\midrule
			1  &  0.684  & 756 \\  
			2  &  0.805  & 1,106 \\  
			3  &  0.863  & 1,444 \\  
			4  &  0.879  & 1,717 \\  
			5  &  0.857  & 1,919 \\  
			6  &  0.931  & 2,108 \\  
			7  &  0.950  & 2,251 \\  
			8  &  0.958  & 2,353 \\  
			9  &  0.966  & 2,432 \\  
			\bottomrule
		\end{tabular}
	\end{center}    
\end{table}


During the execution of the experiment we notice that the classification can vary accordingly with the project being classified and the data used as test. We also intuitively know that developers from different projects express them selfs differently. These characteristics can be observed during the iterative classification where the addition of more comments not necessarily implies in a improvement on the F1 measure.

One of the reasons for this is that the weight of features is given trough empirical probability, and consequently features that appears with more frequency will have a higher weight. Although this is an effective process for the majority of cases that we studied, it can be misleading when classifying comments that has different context. With smaller datasets such as the training data for requirement \SATD, the addition of comments that does not represent the way that developers write comments in the classified project will actually decrease the F1 measure performance. 

\conclusionbox{We find that the design \SATD comments can be classified effectively using a training dataset of 1,444 these comments. Similarly requirement \SATD can be classified building a training dataset with 1,055 comments of this category.}