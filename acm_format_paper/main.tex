\documentclass{sig-alternate}\usepackage{amssymb,amsmath}\usepackage{wrapfig}\usepackage{multirow}\usepackage{graphicx}\usepackage{algorithm}\usepackage{algorithmic}\usepackage{times}\usepackage{cite}\usepackage{url}\usepackage{booktabs}\usepackage{subfigure}\usepackage{fancybox}\usepackage{color}\usepackage{array}\usepackage{subfigure}\usepackage{balance}\usepackage{epstopdf}\usepackage{array}\usepackage{xspace}\usepackage{makecell}\usepackage{siunitx}% author comments with colors \newcommand{\emad}[1]{\textcolor{red}{{\it [Emad: #1]}}}\newcommand{\nikos}[1]{\textcolor{red}{{\it [Nikos: #1]}}}\newcommand{\everton}[1]{\textcolor{blue}{{\it [Everton: #1]}}}\newcommand{\todo}[1]{\colorbox{yellow}{\textbf{[#1]}}}% conclusion box for summarize the research questions\newcommand{\conclusionbox}[1]{%       \vspace{2mm}       \framebox[0.45\textwidth][c]{%              \parbox[b]{0.42\textwidth}{%                     {\it #1}              }       }       \vspace{2mm}}% research questions \newcommand{\rqi}{\textbf{RQ1. Is it possible to effectively identify/predict \SATD using NLP techniques ?\\}}\newcommand{\rqii}{\textbf{RQ2. What are the most common comment patterns that indicates \SATD ?\\}}\newcommand{\rqiii}{\textbf{RQ3. How much training data is necessary for successfully predict \SATD ?\\}}% commands for common terms in the text\newcommand{\SATD}{self-admitted technical debt\xspace}\begin{document}% --- Author Metadata here ---% \conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.% --- End of Author Metadata ---\title{Using Source Code Comments to Detect Self-Admitted Technical Debt}\numberofauthors{1} \author{\alignauthor        Everton da S. Maldonado, Nikolaos Tsantalis and Emad Shihab\\       \affaddr{Department of Computer Science and Software Engineering}\\       \affaddr{Concordia University, Montreal, Canada }\\       \email{e\_silvam@encs.concordia.ca, nikolaos.tsantalis@concordia.ca, emad.shihab@concordia.ca}% 2nd. author% \alignauthor %        Second Author\\%        \affaddr{Department of Computer Science and Software Engineering}\\%        \affaddr{Concordia University}\\%        \affaddr{Montreal, Canada}\\%        \email{2autor@encs.concordia.ca}% % 3rd. author% \alignauthor %        Third author\\%        \affaddr{Department of Computer Science and Software Engineering}\\%        \affaddr{Concordia University}\\%        \affaddr{Montreal, Canada}\\%        \email{3author@encs.concordia.ca}}\maketitle\begin{abstract}During the development and maintenance of a software system, developers face unpredictable difficulties or pressures, and in many cases are forced to apply unconventional solutions to overcome these difficulties. For example, they might adopt insufficiently tested or temporary solutions (i.e., workarounds and hacks), neglect good design practices, and introduce inaccurate or incomplete documentation due to time constraints and pressure to meet deadlines. This phenomenon has been explained through the metaphor of Technical Debt. More recently, our work shown that one possible source to detect technical debt is using source code comments, also referred to as self-admitted technical debt. In addition to that, we pointed out that the most common types of self-admitted technical debt found in comments are design technical debt, requirement debt and defect debt. Therefore, in this paper we present an approach to efficiently identify these types of self-admitted technical debt using a maximum entropy classifier. % More specifically,  using natural language processing techniques. \end{abstract}% A category with the (minimum) three required fields% \category{H.4}{Information Systems Applications}{Miscellaneous}%A category including the fourth, optional field follows...% \category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]\terms{}\keywords{}\section{Introduction}\label{sec:introduction}Developers often have to deal with conflicting goals that require software to be delivered quickly, with high quality, and on budget. In practice, achieving all of these goals at the same time can be challenging, causing a tradeoff to be made. Often, these tradeoffs lead developers to take \emph{shortcuts} or use \emph{workarounds}. Although such shortcuts help developers in meeting their short-term goals, they may have a negative impact in the long-term.Technical debt is a metaphor that has been used to express sub-optimal solutions that are taken consciously in a software project in order to achieve some short-term goals. Generally, these decisions allow the project to move faster in the short-term, but introduce an increased cost (i.e., debt) to maintain this software in the long run~\cite{Seaman2011,Kruchten2013IWMTD}. Prior work showed that technical debt is widespread in the software domain, is unavoidable, and can have a negative impact on the quality of the software~\cite{Lim2012Software}.Due to the importance of technical debt, a number of studies empirically examined it and proposed techniques to enable its detection and management. The main findings of the prior work is that 1) there are different types of technical debt, e.g., defect debt, design debt, testing debt, and that design debt has the highest impact~\cite{Alves2014MTD,Marinescu2012IBM}; and 2) statically analyzing the source code can help detecting technical debt~\cite{Marinescu2004ICSM,Marinescu2010CSMR,Zazworka2013CSE}. In particular, these works use metric thresholds to detect code smells, which are considered as proxies for technical debt. One major drawback of using metrics to detect technical debt is that no one knows if the detected smells really constitute technical debt, or if they correspond to problems that the developers care about. Therefore, more recently, our work showed that using code comments can be effective in identifying self-admitted technical debt~\cite{Potdar2014ICSME}. This work uses comments to detect \emph{generic} technical debt, and did not focus on any specific type of technical debt. Based on this knowledge, Maldonado and Shihab~\cite{Maldonado2015MTD} showed that \emph{specific} types of self-admitted technical debt (i.e., design debt, requirement debt, defect debt, documentation debt and test debt) can be identified in code comments. Accordingly with their findings, the two most common types of \SATD comments are design debt and requirement debt. Then, they made this classified dataset of code comments publicly available for use. However, thus far relying on manual classification of \SATD comments renders the approach not appealing for use.Therefore, inspired in these previous work, in this paper we analyze the efficiency of Natural Language Processing techniques to detect the two most common types of \SATD comments, design debt and requirement debt. In order to do that, we analyze ten open source projects namely Apache Ant, Apache Jmeter, ArgoUML, Columba, EMF, Hibernate, JEdit, JFreeChart, JRuby and SQuirrel. We first extract the source comments of these projects and filtered them removing comments that are not likely to have \SATD. The remainder of the source comments were manually classified into the different types of technical debt whenever was possible. Combining the dataset from our previous work with this one we collected more than \todo{}. With our filtering heuristics we reduced this number to \todo{}, and we manually classified them. Using this dataset we use the Stanford Classifier, a maximum entropy classifier tool, to predict (i.e., classify) \SATD.We find that NLP techniques, such as maximum entropy classifiers, can be used effectively to find \SATD comments. We achieved an average F1 measure of \todo{} for design debt, an average of \todo{} while classifying requirement debt. We also analyzed which are the more common words used to classify \SATD comments. We find that words such as `hack', `workaround', `yuck!', `kludge', `stupidity', `needed?', `unused?', `columns??', `FIXME:' and `wtf?' are a strong indicative of design debt. Whereas, `TODO:', `FIXME:', `needed', `implementation', `ends?', `apparently', `XXX', `configurable', `Auto-generated' and `empty' are a strong indicative of requirement debt.Then we turn our focus in how much training data is necessary to effectively identify \SATD. We find that training datasets with at least 1,444 design debt comments can achieve good performance. Similarly, for requirement debt, training datasets with at least 1,055 source comments of this type can perform well.  Lastly, we made the dataset used in this work public available. We believe that this is a good contribution to future work in the area, and to researchers who are whiling to devise new approaches to address the problem of identifying technical debt trough the use of code comments.The rest of the paper is organized as follows. Section \ref{sec:approach} describes our approach. We setup our case study and present ourresults in Section \ref{sec:case_study_results}. In Section \ref{sec:related_work} we show related previous work. Section \ref{sec:threats_to_validity} present the threats to validity and Section \ref{sec:conclusion} concludes our study.  %\include{introduction}%\section{Motivating Example}%\label{sec:motivating_example}%\input{motivation}\section{Approach}\label{sec:approach}\input{approach}\section{Case study Results}\label{sec:case_study_results}\input{results}%\section{Discussion}%\label{sec:discussion}%\input{discussion}\section{Related Work}\label{sec:related_work}\input{related_work}\section{Threats to validity}\label{sec:threats_to_validity}\input{threats_to_validity}\section{Conclusion and Future work}\label{sec:conclusion}\input{conclusion}\bibliographystyle{abbrv}\bibliography{bibliography}  \include{appendix}\end{document}