\documentclass{sig-alternate}\usepackage{amssymb,amsmath}\usepackage{wrapfig}\usepackage{multirow}\usepackage{graphicx}\usepackage{algorithm}\usepackage{algorithmic}\usepackage{times}\usepackage{cite}\usepackage{url}\usepackage{booktabs}\usepackage{fancybox}\usepackage{color}\usepackage{array}\usepackage{subfigure}\usepackage{balance}\usepackage{epstopdf}\usepackage{array}\usepackage{xspace}\usepackage{makecell}\usepackage{siunitx}\usepackage{caption}% author comments with colors \newcommand{\emad}[1]{\textcolor{red}{{\it [Emad: #1]}}}\newcommand{\nikos}[1]{\textcolor{red}{{\it [Nikos: #1]}}}\newcommand{\everton}[1]{\textcolor{blue}{{\it [Everton: #1]}}}\newcommand{\todo}[1]{\colorbox{yellow}{\textbf{[#1]}}}% conclusion box for summarize the research questions\newcommand{\conclusionbox}[1]{%       \vspace{2mm}       \framebox[0.45\textwidth][c]{%              \parbox[b]{0.42\textwidth}{%                     {\it #1}              }       }       \vspace{2mm}}% research questions \newcommand{\rqi}{\textbf{RQ1. Is it possible to effectively identify/predict \SATD using NLP techniques?\\}}\newcommand{\rqii}{\textbf{RQ2. What are the most impacting words in the classification \SATD?\\}}\newcommand{\rqiii}{\textbf{RQ3. How much training data is necessary for successfully predict \SATD?\\}}% commands for common terms in the text\newcommand{\SATD}{self-admitted technical debt\xspace}\hyphenation{work-arounds}\begin{document}% --- Author Metadata here ---% \conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.% --- End of Author Metadata ---\title{Using Source Code Comments to Automatically Detect Self-Admitted Technical Debt}\numberofauthors{1} \author{\alignauthor        Everton da S. Maldonado, Nikolaos Tsantalis and Emad Shihab\\       \affaddr{Department of Computer Science and Software Engineering}\\       \affaddr{Concordia University, Montreal, Canada }\\       \email{e\_silvam@encs.concordia.ca, nikolaos.tsantalis@concordia.ca, emad.shihab@concordia.ca}}\maketitle\begin{abstract}During the development of a software system developers face unpredictable difficulties, and in many cases they apply unconventional solutions such as hacks and workarounds. The \textit{metaphor technical debt} was created to express this trade off between productivity and quality. More recently, our work has shown that it is possible to detect technical debt using source code comments (i.e., self-admitted technical debt), and that the most common types of self-admitted technical debt are design and requirement debt. However, this approach heavily depends on the manual classification of source comments. In this paper we present an approach to automatically identify design and requirement \SATD using a maximum entropy classifier. We studied ten open source projects: Apache Ant, Apache Jmeter, ArgoUML, Columba, EMF, Hibernate, JEdit, JFreeChart, JRuby and SQuirrel. We find that the average F1 measure achieved when identifying \SATD outperforms the state-of-the-art technique average F1 measure 1.9 to 20 times. During our analysis we evaluate and present the top 10 most common words that developers use to express technical debt. Lastly, we find that training datasets as small as 1,444 design debt comments and 601 requirement debt comments can be used to identify \SATD comments. To facilitate future work we make publicly available the dataset of classified comments used in this study containing more than 63K comments. \end{abstract}\terms{}\keywords{}\section{Introduction}\label{sec:introduction}Developers often have to deal with conflicting goals that require software to be delivered quickly, with high quality, and on budget. In practice, achieving all of these goals at the same time can be challenging, causing a tradeoff to be made. Often, these tradeoffs lead developers to take \emph{shortcuts} or use \emph{workarounds}. Although such shortcuts help developers in meeting their short-term goals, they may have a negative impact in the long-term.Technical debt is a metaphor coined to express sub-optimal solutions that are taken consciously in a software project in order to achieve some short-term goals~\cite{Cunningham1992WPM}. Generally, these decisions allow the project to move faster in the short-term, but introduce an increased cost (i.e., debt) to maintain this software in the long run~\cite{Seaman2011,Kruchten2013IWMTD}. Prior work has shown that technical debt is widespread in the software domain, is unavoidable, and can have a negative impact on the quality of the software~\cite{Lim2012Software}.Due to the importance of technical debt, a number of studies empirically examined technical debt and proposed techniques to enable its detection and management. The main findings of the prior work are that 1) there are different types of technical debt, e.g., defect debt, design debt, testing debt, and that among them design debt has the highest impact~\cite{Alves2014MTD,Marinescu2012IBM}. 2) Static source code analysis helps detecting technical debt, (i.e., code smells)~\cite{Marinescu2004ICSM,Marinescu2010CSMR,Zazworka2013CSE}. 3) More recently, our work shown that it is possible to identify technical debt through source comments (i.e., \SATD)~\cite{Potdar2014ICSME}, and that design and requirement debt are the most common types of \SATD~\cite{Maldonado2015MTD}.The recovery of technical debt through source code comments has three main advantages over traditional approaches based on source code analysis and metric rules to detect code smells.First, it is more lightweight compared to source code analysis, since it does not require the construction of Abstract Syntax Trees or other more advanced source code representations, such as control flow graphs, and program dependence graphs in order to match structural code smell patterns and compute metrics.The source code comments can be easily and efficiently extracted from source code files using regular expressions.Second, it does not depend on arbitrary metric threshold values, which are required in all metric-based code smell detection approaches.Deriving appropriate threshold values is a challenging open problem that has attracted the attention and effort of several researchers~\cite{Oliveira:2014,Fontana:2015,Fontana:EMSE:2015}.Finally, and more importantly, it is a more reliable approach for the recovery of technical debt, since it is based on comments written by the developers themselves, who admit the existence of technical problems that should be addressed in the future.On the contrary, all approaches based on source code analysis suffer from high false positive rates~\cite{Ferme:2013} (i.e., they flag a large number of source code elements as problematic, while they are not perceived as such by the developers), because they rely only on the structure of the source code to detect code smells without taking into account the developers' opinion and the project domain.Despite the advantages of recovering technical debt from source code comments, the research in \SATD, thus far, heavily relies on manual inspection of code comments. The manual inspection of code comments is subject to the reader bias, it is time consuming and, as any other manual task, susceptible to errors. These limitations in the identification of \SATD comments renders the current approach difficult to be applied.In this paper we investigate the efficiency of Natural Language Processing techniques to automatically detect the two most common types of \SATD comments, design and requirement debt. We analyze ten open source projects from different application domains, namely, Apache Ant, Apache Jmeter, ArgoUML, Columba, EMF, Hibernate, JEdit, JFreeChart, JRuby and SQuirrel. We extract and classify the source comments of these projects. Then, using the classified dataset we execute a maximum entropy classifier tool (i.e., the Stanford Classifier)~\cite{Manning2014ACL}, to identify design and requirement \SATD comments.We achieved an average F1 measure of 0.620 while identifying design \SATD, and an average F1 measure of 0.509 while identifying requirement \SATD. Which means that our performance is on average 8.6 times better than the current state-of-the-art approach to identify design \SATD~\cite{Potdar2014ICSME}. The current-state-of-the art approach uses 64 comment patterns (i.e., words and phrases) derived after the manual examination of more than 100k comments.% , and 29.9 times better identifying requirement \SATD commentsTo better understand how developers express technical debt we analyze the 10 most prevalent words in \SATD comments. For example, words as `hack', `workaround' and `yuck!' are used to express design \SATD. Whereas, `todo', `needed' and `implementation' are a strong indicators of requirement debt.Then, to determine the most efficient way to apply our approach we analyze the amount of training data necessary to identify \SATD. We find that training datasets with at least 1,444 design \SATD comments can achieve good prediction performance. Similarly, for requirement \SATD, training datasets with at least 601 source comments containing requirement \SATD can identify this type of technical debt with success.  The contributions of our work are the following:\begin{enumerate}	\item We provide a automatic approach for identifying design and requirement \SATD.	\item We analyze the amount of necessary data to effectively apply our approach, making it possible to replicate our study in different contexts than ours (i.e., different idioms or programming languages).	\item We made the dataset used in this work publicly available. We believe that these are good contributions to future work in the area, and to researchers who are willing to devise new approaches to address the problem of identifying technical debt through the use of code comments.\end{enumerate}% We also made the dataset used in this work publicly available. We believe that this is a good contribution to future work in the area, and to researchers who are whiling to devise new approaches to address the problem of identifying technical debt through the use of code comments.The rest of the paper is organized as follows. Section \ref{sec:approach} describes our approach. We setup our case study and present ourresults in Section \ref{sec:case_study_results}. We discuss the implication of our findings in Section \ref{sec:discussion}. In Section \ref{sec:related_work} we show related previous work. Section \ref{sec:threats_to_validity} present the threats to validity and Section \ref{sec:conclusion} presents our conclusions and future work.  \section{Approach}\label{sec:approach}\input{approach}\section{Case study Results}\label{sec:case_study_results}\input{results}\section{Discussion}\label{sec:discussion}\input{discussion}\section{Related Work}\label{sec:related_work}\input{related_work}\section{Threats to validity}\label{sec:threats_to_validity}\input{threats_to_validity}\section{Conclusion and Future work}\label{sec:conclusion}\input{conclusion}\bibliographystyle{abbrv}\bibliography{bibliography}  \include{appendix}\end{document}