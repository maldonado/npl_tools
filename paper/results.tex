
The main goal of this study is to determine if comments in the source code can be used to identify \emad{self-admitted?} Design Technical Debt. We perform an exploratory study using the data of ten open source projects namely - Apache Ant, Apache Jmeter, ArgoUML, Columba, EMF, Hibernate, JEdit, JFreeChart, JRuby and SQuirrel SQL Client. In particular, we would like to 1) develop a taxonomy to help us identify design debt from the code comments, 2) determine the effectiveness of our taxonomy at detecting design debt and 3) quantify how much of the self-admitted design debt we can automatically refactor with refactoring tools.  \emad{This should be moved earlier in the approach section maybe.}

\rqi

\noindent \textbf{Motivation:} 

\begin{itemize}
\item prior work showed that comments are good indicators of technical debt
\item However, we know there are different types, so we would like to propose comment patterns that are specific to design debt.
\item Therefore, our first task is to derive a set of comment patterns that indicate \SADTD
\end{itemize}

As shown in a previous study, self-admitted technical debt can be found in the comments of a project \cite{Potdar2014ICSME}. However  technical debt is a broad concept, and classifying it into more specific categories will allow us to address the occurrences of technical debt in a more efficient way \emad{how?}. Finding and quantifying these words is the first step. We want to know which are the words that accurately identifies design technical debt and how can we extract them. 

\noindent \textbf{Approach:} To determine what comment patterns best indicate \SADTD, we use the approach shown in \todo{Section}~\ref{sec:approach}. In a nut shell, we started by using common words that prior work~\cite{fowler1999refactoring, brown1998antipatterns,martin2009clean} associated with design paradigms (e.g., \todo{add sample words}). Since using this list did not yield favourable results, we manually examined source code comments of the Ant project. To minimize the large cost of manual examination, we used a number of heuristics that reduced the set of comments to inspect to a reasonable set. 

In total, the Apache Ant project 21,587 comments across in 1475 Java classes. After applying our heuristics, we ended up with a total of 4,678 comments that required manual inspection. After the manual inspection of the 4,678 comments,  we ended up with a final list of 81 \SADTD. \emad{not sure if we should add this: During this classification we focused specifically in design technical debt, other kinds of technical debt, when found, were classified as without classification} The entire process of determining these \SADTD comment patterns took one masters student (the first author) approximately 32 hours to complete. 

%To find what words indicates self-admitted design technical debt we use 
%two different approaches. First, we \emad{try?} to suggest words based in terms that are used in the literature \cite{fowler1999refactoring}\cite{brown1998antipatterns}\cite{martin2009clean}, in the research community and our own experience. Second, extract and analyze all comments from a project, classifying the ones that represent design technical debt and then, search for patterns between them. We apply several queries in the database, one for each of the words in the dictionaries. In our queries we are ignoring case sensitivity and we also use wild cards in the search, which means that we are searching for the specific word within the entire content of the comment. \emad{do we have those words listed in the paper? We should also say what we applied these patterns on}
%Using wild cards allows us to search for similar words using the same query, for example one query using the word "\%dependen\%" would result in positive results for comments that contains the words "dependency" or "dependencies".

%In order to execute the second approach \emad{it?} is necessary a classified dataset to look for patterns in the design technical debt comments. To the best of our knowledge, there is not a dataset with that data  available so we choose  a project, Apache Ant, and manually classified it. Before executing the heuristics \emad{mention the heuristics explicitly} Apache Ant had 21587 comments distributed in 1475 Java classes, after the execution of the heuristics and the post-processing technique the remaining comments were 4678. That was the total number of comments that we manually classified \emad{elaborate more on this...there is a lot of work that went into this.}. The classification took a total time of 32 hours of one master student who has industrial experience in the Java programming language. The comments were classified into the following categories: "design\_related" or  "whitout\_classification" \emad{what is the other option man?... this is super obvious}. During this classification we focused specifically in design technical debt, other kinds of technical debt, when found, were classified as "without\_classification". 

\noindent \textbf{Results:} 

Table \todo{add table} shows the list of comment patterns that indicate \SADTD. In total, we determined a total of 81 unique comment patterns. In addition to the unique comment patterns, we also combined patterns to come up with more descriptive comment patterns of \SADTD. In total, we had 175 different comment patterns that indicate \SADTD. 

We also compare the comment patterns derived here to the more general comment patterns derived by Potdar and Shihab~\todo{cite Potdar and Shihab} used to determine self-admitted technical debt in general (i.e., design debt, defect debt, testing debt and so forth). Comparing to Table \todo{Add Potdar and Shihab's table}, the comment patterns indicating \SADTD are different. This observation shows that although comments are good indicators of self-admitted technical debt \todo{cite Potdar}, different types of technical debt are indicated by different comment patterns.

\emad{Add a lit of the most common patterns}
In addition to determining the comment patterns that indicate \SADTD, we also investigated the most common \SADTD comment patterns. Our findings show that the top 5 most common \SADTD comment patters are: \todo{list the most common terms and their percentage}. We see that the top 5 most common comment patterns indicate the \emad{majority?} of the \SADTD, accounting for more than \todo{add number\%} of the \SADTD occurrences.


%The first approach described generated 3 different dictionaries that we evaluated separately \emad{you need to elaborate on the fact that you had 3 dictionaries in the approach...}. The "-ilities" dictionary containing 18 words like maintainability, flexibility, portability, etc. the "Design" dictionary which has 14 words and the "Bad Smells" dictionary with 8 words. 
%
%
%Using the classified dataset, we were able to create a new dictionary that contains 81 different words \emad{don't say words. Call them comment patterns...just sounds more formal}, that we call root words, the root words are combined in a strategical way to generate 175 different combinations that can identify self-admitted design technical debt. Part of these 81 words were obtained using the previous dictionaries. The false positives could be drastically reduced by the use of the combinations of words.
%
%After executing the second approach, we noticed that is much more efficient to combine words into expressions than use single words. We also find that looking into classified data and then searching for patterns has shown better results than suggesting words. \emad{I don't understand the last sentence}
%

\conclusionbox{FILL THIS IN LATER, WHEN THE RESULTS ARE IN}


\rqii

\noindent \textbf{Motivation:} Just identify the words that can represent design technical debt is not enough. In order to make a real collaboration \emad{collaboration?} in this subject we need to know how good our approach is in finding design technical debt. We want to know exactly how many comments we can correctly identify and how many comments could have been identified in an ideal situation. Only then we will be able to know if our proposed approach is viable and if it is retrieving interesting results.  

\emad{We manually examined the comments to determine design debt. However, we need to determine the effectiveness of these words. If the words are too general, then we will have many false positives leading to a waste of effort. If it is too restrictive, then we will miss many of the actual \SADTD. Therefore, our goal here is to find out how well our \SADTD comment patterns do at detecting \SADTD.}

\par \noindent \textbf{Approach:} To quantify the effectiveness of our technique, we want to evaluate the results of our dictionary  using precision and recall values. As there is no automated way to identify if a found comment is in fact a match for design technical debt and, to the best of our knowledge to the present date there is not an available dataset containing the required information that we need to do this assertion, we manually classified all the comments of three well-know projects: Apache-ant 1.7.0, Apache JMeter 2.10 and JFreeChart 1.0.19. In order to classify this data set we first remove the unnecessary comments by running the heuristics and the post-processing technique. \emad{why these two projects? Why is this different from RQ1?}

For Apache Ant, before we execute the heuristics we had 21587 comments distributed in 1475 Java classes, after the execution of the heuristic the remaining comments were 4678. That was the total number of comments that we manually classified. The classification took a total time of 32 hours of one master student who has industrial experience in the Java programming language. \emad{I don't think you need to talk about the heuristics here.}

For Apache Jmeter, the reaming  comments to manually analyze were 8102, The classification took a total time of 54 hours of one master student who has industrial experience in the Java programming language. Finally the number of manually canalized comments for JFreeChart  were 4452 and it took 29 hours to the same student to classify it. Regardless of the project, the comments were classified into the following categories: "design\_related" or "whitout\_classification". During this classification we focused specifically in design technical debt, other kinds of technical debt, when found, were classified as "without\_classification". 

\emad{I am thinking of maybe combining RQ1 and RQ2...The approach seems very similar and they seem very related in general...}

\noindent \textbf{Results:} Our first analysis aims only the comments found in the Apache-ant project, as we have the classified dataset to validate precision and recall \emad{not sure what this sentence means}. Table 4 \emad{reference properly} shows the results for the three proposed dictionaries. For the "-ilities" dictionary we matched 10 comments but only 2 were real design technical debt comments. The recall is even lower as we found 2 matches out of 93 available. For the "design dictionary" we matched 54 comments, of them 5 represented a design technical debt comment. Finally we found 39 comments using the "bad smells" dictionary but comparing with the our manual classification only one represented a design technical debt.

We find that even with the improvement that the heuristic did to the dataset removing good part of the false positives we still do not have a good precision with any of the proposed dictionaries. The precision and recall were respectively 20\% and 2.15\%, 9.26\% and 5.38\% and finally 2.56\% and 1.01\%.




\begin{table}[!hbt]
    \begin{center}
        \caption{Dictionaries Evaluation}
        \label{tab:dictionaryEvaluation}
        \begin{tabular}{l| c c c c} 
            \toprule
            \textbf{Dictionary} & \textbf{Found} &  \textbf{Match}  &  \textbf{Precision} & \textbf{Recall} \\ 
            \midrule
            "-ilities" & 10 & 2 & 20\% & 2.15\% \\ 
            Design & 54 & 5 & 9.26\% & 5.38\% \\
            Bad Smell  & 39 & 1 & 2.56\% & 1.01\% \\ 
            \bottomrule
        \end{tabular}
    \end{center}
\end{table}

\begin{table}[!hbt]
    \begin{center}
        \caption{Expression Dictionary}
        \label{tab:dictionaryExpression}
        \begin{tabular}{l| c c c c} 
            \toprule
            \textbf{Dataset} & \textbf{Found} &  \textbf{Match}  &  \textbf{Precision} & \textbf{Recall} \\
            \midrule
            Apache-Ant    & 81 & 78 & 96.30\% & 83.87\% \\ 
            Apache-Jmeter & 75 & 66 & 88.00\% & 27.16\% \\ 
            JFreeChart    & 12 & 10 & 83.33\% & 10.87\% \\ 
            All Projects  & 964 & 825 & 85.58\% & - \\  
            \bottomrule
        \end{tabular}
    \end{center}
\end{table}


After the conception of the expression dictionary, we first evaluate the precision and recall in the Apache-ant project. As the project was mainly used to compose the words in the dictionary we expect precision and recall values to be very high. We find that out of the 81 comments that was matched 78 was related with \emad{self-admitted?} design technical debt. The precision of this new dictionary was 96.30\% and the recall 83.87\%. We were impressed with the results because of the improvement in the results \emad{please remove this sentence...we cannot say that we are awesome}. 

We analyzed the performance of the dictionary in the database that contained the comments of all of ten used projects. As all the classification done so far has to be manual, the effort needed to classify all the comments of all the projects  in the same way that we did for Apache Ant, Apache JMeter and JFreeChart projects would be unfeasible due time constrains. So we decided to measure just precision in this case. 

Nevertheless, we manually checked the 964 comments that were matched when using the dictionary in this dataset. The number of positive matches if the manual analysis was 825 comments, with means a precision of 85.58\%. The analysis took 8 hours of work of one master student. Refer to Table 5 \emad{reference properly} for the results.


%\vspace{5mm} 
%\par \noindent \textbf{RQ1.} What words indicate self-admitted design technical debt?
%\vspace{5mm} 
%\par \textit{Our results shows that the following words and combinations of words has a high precision identifying design technical debt. Future, Quick Fix, Temporary, Place some where else, Used other place, It may change, Not sure, Dependency Cycle, Code copied from, Any reason, Wrong Place, Hairy, Instead-Could, Ugly, Avoid, Pathological, Stolen, Not Well Formed, No Sense Since, Without noticing, Brittle, Really Necessary, Cares, No Idea, Doing, Elsewhere, Rather complex, Held, Though unused, Don't Know, Not fond, More elegant, Clean Way, Remove, Don't want, Fix for, Irritating, Duplication), Why not, Rethink, Rework, Pointless, Not nice, Hack, Only Developer Knows, Use help, Hammer, Redundant, For some reason, Alternatively-Could, Technically, Forces us, Better Way, Hard Coded, Kludge, Wrong visibility, Messy, Should-Instead, Weird, Availability, Extensibility, Flexibility, Scalability, Security, Ambiguous, Big, Clean, Complex, Consistency-Sake, Lack, Long, Large, Maintenan ce, Unused, bad, Clone, Dead Code, Design, Magic and  Smell.}


%\vspace{5mm} 
%\par \noindent \textbf{RQ2.} How effective is the proposed technique at detecting self-admitted design technical debt?
%\vspace{5mm} 

\conclusionbox{We find that our expression dictionary obtained a precision of 96.30\% and recall of 83.87\% in the Apache-ant project, for the Apache JMeter and JFreeChart projects we have 88.00\% precision 27.16\% recall and 83.33\% precision 10.87\% recall respectively. Finally when  using the whole dataset the precision obtained was of 85.58\%.}
