% -*- root: main.tex -*-
The goal of our research is to develop an automatic way to detect design and requirement \SATD comments. To do so, we first manually classify a large number of comments identifying those containing \SATD. With the resulting dataset, we train the \revised{maximum entropy classifier}{R2-11} to identify design and requirement \SATD (RQ1). To better understand what words indicate \SATD, we inspect the features used by the \revised{maximum entropy classifier}{R2-11} to identify the detected \SATD. These features are words that are frequently found in comments with technical debt. We present the 10 most common words that indicate design and requirement \SATD (RQ2). Since the manual classification required to create our training dataset is expensive, ideally we would like to achieve maximum performance with the least amount of training data. Therefore, we investigate how variations in the size of training data affects the performance of our classification (RQ3). We detail the motivation, approach and present the results of each of our research questions in the remainder of this section. 

\begin{figure*}[!thb]
  \centering
  \subfigure[Design Debt]{\includegraphics[width=0.48\textwidth]{figures/f1_measure_comparisom_design_1.pdf}
  \label{fig:f1_measure_comparison_design_debt}}
  \subfigure[Requirement Debt]{\includegraphics[width=0.48\textwidth]{figures/f1_measure_comparisom_requirement_1.pdf}
  \label{fig:f1_measure_comparison_requirement_debt}}
  \vspace{-3mm}
  \caption{Visualization of the F1-measure for Different Approaches}
\end{figure*}

\begin{table*}[!thb]
    \begin{center}
        \caption{Comparison of F1-measure between the NLP-based, the Comment Patterns and the Random Baseline Approaches for Design and Requirement Debt}
        \label{tbl:improvement_f1measure}
        \begin{tabular}{l| c c c c c| c c c c c}
        \toprule
        
        % draw first line. The * centralizes the Project column, then set the total size of columns that we have
        \multirow{5}{*}{\textbf{\thead{Project}}} & \multicolumn{5}{c|}{\textbf{\thead{Design debt}}} & \multicolumn{5}{c}{\textbf{\thead{Requirement debt}}} 
        % indicates that from now on we are filling the content of the next line
        \\ 
        \cmidrule{2-6}
        \cmidrule{7-11}
        % remainder columns
        & {\textbf{\thead{Our\\approach}}} & {\textbf{\thead{Comment\\patterns}}} & {\textbf{\thead{Random\\classifier}}} & {\textbf{\thead{IMP over\\comment\\patterns}}} & {\textbf{\thead{IMP over\\random\\classifier}}} & {\textbf{\thead{Our\\approach}}} & {\textbf{\thead{Comment\\patterns}}} & {\textbf{\thead{Random\\classifier}}} & {\textbf{\thead{IMP over\\comment\\patterns}}} & {\textbf{\thead{IMP over\\random\\classifier}}} \\
  
        \midrule                                                  
        \textbf{Ant}       &0.517&0.237&0.044&2.1$\times$& 11.7 $\times$&0.154&0.000&0.006&-             & 25.6$\times$  \\
        \textbf{ArgoUML}   &0.814&0.107&0.144&7.6$\times$& 5.6  $\times$&0.595&0.000&0.079&-             & 7.5 $\times$  \\
        \textbf{Columba}   &0.601&0.264&0.037&2.2$\times$& 16.2 $\times$&0.804&0.117&0.013&6.8  $\times$ & 61.8$\times$  \\
        \textbf{EMF}       &0.470&0.231&0.034&2.0$\times$& 13.8 $\times$&0.381&0.000&0.007&-             & 54.4$\times$  \\
        \textbf{Hibernate} &0.744&0.227&0.193&3.2$\times$& 3.8  $\times$&0.476&0.000&0.041&-             & 11.6$\times$  \\
        \textbf{JEdit}     &0.509&0.342&0.037&1.4$\times$& 13.7 $\times$&0.091&0.000&0.003&-             & 30.3$\times$  \\
        \textbf{JFreeChart}&0.492&0.282&0.077&1.7$\times$& 6.3  $\times$&0.321&0.000&0.007&-             & 45.8$\times$  \\
        \textbf{Jmeter}    &0.731&0.194&0.072&3.7$\times$& 10.1 $\times$&0.237&0.148&0.005&1.6 $\times$  & 47.4$\times$  \\
        \textbf{JRuby}     &0.783&0.620&0.123&1.2$\times$& 6.3  $\times$&0.435&0.409&0.043&1.0 $\times$  & 10.1$\times$  \\
        \textbf{SQuirrel}  &0.540&0.175&0.055&3.0$\times$& 9.8  $\times$&0.541&0.000&0.014&-             & 38.6$\times$  \\
        \midrule 
        \textbf{Average}   &0.620&0.267&0.081&2.3$\times$&7.6   $\times$&0.403&0.067&0.021&6.0 $\times$  & 19.1$\times$ \\ 
        \bottomrule
        \end{tabular}
    \end{center}    
\end{table*}

\vspace{3mm}
\noindent\rqi
\vspace{3mm}

\noindent \textbf{Motivation:} As shown in previous work~\cite{Maldonado2015MTD}, \SATD comments can be found in the source code. However, there is no automatic way to identify these comments. The methods proposed so far heavily rely on the manual inspection of source code, and there is no evidence on how well these approaches perform. Moreover, most of them do not discriminate between the different types of technical debt (e.g., design, test, requirement).

Therefore, we want to determine if NLP techniques such as, the \revised{maximum entropy classifier}{R2-11}, can help us surpass these limitations \revised{and outperform the accuracy of the current state-of-the-art}{R1-7}. The \revised{maximum entropy classifier}{R2-11} can automatically classify comments based on specific linguistic characteristics of these comments. Answering this question is important, since it helps us understand the opportunities and limitations of using NLP techniques to automatically identify \SATD comments. 

\vspace{1mm}
\noindent \textbf{Approach:} For this research question, we would like to examine how effectively we can identify design and requirement \SATD. Therefore, the first step is to create a dataset that we can train and test the \revised{maximum entropy classifier}{R2-11} on. We classified the source code comments into the following types of \SATD: design, defect, documentation, requirement, and test debt. However, our previous work showed that the most frequent \SATD comments are design and requirement debt. Therefore, in this paper, we focus on the identification of these two types of \SATD, because 1) they are the most common types of technical debt, and 2) NLP-based techniques require sufficient data for training (i.e., they cannot build an accurate model with a small number of samples).

We train the \revised{maximum entropy classifier}{R2-11} using our manually created dataset. The dataset contains comments with and without \SATD, and each comment has a classification (i.e., without technical debt, design debt, or requirement debt). Then, we add to the training dataset all comments classified as without technical debt and the comments classified as the specific type of \SATD that we want to identify (i.e., design or requirement debt). We use the comments from 9 out of the 10 projects that we analyzed to create the training dataset. The comments from the remaining one project are used to evaluate the classification performed by the \revised{maximum entropy classifier}{R2-11}. We choose to create the training dataset using comments from 9 out of 10 projects, because we want to train the \revised{maximum entropy classifier}{R2-11} with the most diverse data possible (i.e., comments from different domains of applications). However, we discuss the implications of using training datasets of different sizes in RQ3. We repeat this process for each one of the ten projects, each time training on the other 9 projects and testing on the remaining 1 project.

Based on the training dataset, the \revised{maximum entropy classifier}{R2-11} will classify each comment in the test dataset. The resulting classification is compared with the manual classification provided in the test dataset. If a comment in the test dataset has the same manual classification as the classification suggested by the \revised{maximum entropy classifier}{R2-11}, we will have a true positive (tp) or a true negative (tn). True positives are the cases where the \revised{maximum entropy classifier}{R2-11} correctly identifies \SATD comments, and true negatives are comments without technical debt that are classified as being as such. Similarly, when the classification provided by the tool diverges from the manual classification provided in the test dataset, we have false positives or false negatives. False positives (fp) are comments classified as being \SATD when they are not, and false negatives (fn) are comments classified as without technical debt when they really are \SATD comments. Using the tp, tn, fp, and fn values, we are able to evaluate the performance of different detection approaches in terms of precision ($P=\frac{tp}{tp + fp}$), recall ($R=\frac{tp}{tp + fn}$) and F1-measure ($F=2 \times \frac{P \times R}{P + R}$). To determine how effective the NLP classification is, we compare its F1-measure values with the corresponding F1-measure values of the two other approaches. We use the F1-measure to compare the performance between the approaches as it is the harmonic mean of precision and recall. Using the F1-measure allows us to incorporate the trade-off between precision and recall and present one value that evaluates both measures.

The first approach is the current state-of-the-art in detecting \SATD comments~\cite{Potdar2014ICSME}. This approach uses 62 comment patterns (i.e., keywords and phrases) that were found as recurrent in \SATD comments during the manual inspection of 101,762 comments. The second approach is a simple (random) baseline, which assumes that the detection of \SATD is random (this approach is used as a performance lower bound). The precision of this approach is calculated by taking the total number of \SATD over the total number of comments of each project. For example, project Ant has 4,137 comments, of those, only 95 comments are design \SATD. The chance of randomly finding a design \SATD comment is 0.023 (i.e., $\frac{95}{4,137}$). Similarly, to calculate the recall we take into consideration the two possible classifications available: one is the type of \SATD (e.g., design) and the other is without technical debt. Therefore, there is a 50\% chance that the comment will be classified as \SATD. Thus, the F1-measure for the random baseline for project Ant is computed as $2 \times \frac{0.023 \times 0.5}{0.023 + 0.5} = 0.044$.

\vspace{1mm}

\noindent \textbf{Results - design debt:} Table \ref{tbl:improvement_f1measure} presents the F1-measure of the three approaches, as well as the improvement achieved by our approach compared to the other two approaches. We see that for all projects, the F1-measure achieved by our approach is higher than the other approaches. The F1-measure values obtained by our NLP-based approach range between 0.470 - 0.814, with an average of 0.620. In comparison, the F1-measure values using the comment patterns range between 0.107 - 0.620, with an average of 0.267, while the simple (random) baseline approach achieves F1-measure values in the range of 0.035 - 0.214, with an average of 0.086. Figure \ref{fig:f1_measure_comparison_design_debt} visualizes the comparison of the F1-measure values for our NLP-based approach, the comment patterns approach, and the simple (random) baseline approach. We see from both, Table \ref{tbl:improvement_f1measure} and Figure \ref{fig:f1_measure_comparison_design_debt} that, on average, our approach outperforms the comment patterns approach by 2.3 times and the simple (random) baseline approach by 7.1 times when identifying design \SATD.

It is important to note that the comment patterns approach has a high precision, but low recall, i.e., this approach points correctly to \SATD comments, but as it depends on keywords, it identifies a very small subset of all the \SATD comments in the project. Although we only show the F1-measure values here, we present the precision and recall values in Table~\ref{tbl:classifier_results_vs_baseline_design} in the Appendix section.  

\noindent \textbf{Results - requirement debt:} \revised{Similarly, the last five columns of Table \ref{tbl:improvement_f1measure} show the F1-measure performance of the three approaches, and the improvement achieved by our approach over the two other approaches. The comment patterns approach was able to identify requirement \SATD in only 3 of the 10 analyzed projects. A possible reason for the low performance of the comment patterns in detecting requirement debt is that the comment patterns do not differentiate between the different types of \SATD. Moreover, since most of the debt is design debt, it is possible that the patterns tend to favor the detection of design debt.}{R1-9} 
 
That said, we find that for all projects, the F1-measure values obtained by our approach surpass the F1-measure values of the other approaches. Our approach achieves F1-measure values between 0.091 - 0.804 with an average of 0.403, whereas the comment pattern approach achieves F1-measure values in the range of 0.117 - 0.409 with an average of 0.067, while the simple (random) baseline ranges between 0.003 - 0.083, with an average of 0.022. Figure \ref{fig:f1_measure_comparison_requirement_debt} visualizes the performance comparison of the two approaches.\revised{We also examine if the differences in the F1-measure values obtained by our approach and the other two baselines are statistically significant. Indeed, we find that the differences are statistically significant (p\textless0.5) for both baselines and both design and requirement \SATD.}{R3-6}

Generally, requirement \SATD is less common than design \SATD, which makes it more difficult to detect.\revised{Nevertheless, our NLP-based approach provides a significant improvement over the comment patterns approach, outperforming it by 6 times, on average.}{R1-8} Table~\ref{tbl:improvement_f1measure} only presents the F1-measure values for the sake of brevity, however, we present the detailed precision and recall values in the Appendix section, Table \ref{tbl:classifier_results_vs_baseline_requirement}.

\revised{\conclusionbox{We find that our NLP-based approach, is more accurate in identifying self-admitted technical debt comments compared to the current state-of-art. We achieved an average F1-measure of 0.620 when identifying design debt (an average improvement of 2.3$\times$ over the state-of-the-art approach) and an average F1-measure of 0.403 when identifying requirement debt (an average improvement of 6$\times$ over the state-of-the-art approach).}}{R1-8}

\vspace{3mm}
\noindent\rqii
\vspace{3mm}

\noindent \textbf{Motivation:} After assessing the accuracy of our NLP-based approach in identifying \SATD comments, we want to better understand what words developers use when expressing technical debt. Answering this question will provide insightful information that can guide future research directions, broaden our understanding on \SATD and also help us to detect it.     

\vspace{1mm}
\noindent \textbf{Approach:} The \revised{maximum entropy classifier}{R2-11} learns optimal features that can be used to detect \SATD. A feature is comment fragment (e.g., word) that is associated with a specific class (i.e., design debt, requirement debt, or without technical debt), and a weight that represents how strongly this feature relates to that class. The \revised{maximum entropy classifier}{R2-11} uses the classified training data to determine the features and their weights. Then, these features and their corresponding weights are used to determine if a comment belongs to a specific type of \SATD or not.

For example, let us assume that after the training, the \revised{maximum entropy classifier}{R2-11} determines that the features ``hack'' and ``dirty'' are related to the \textit{design-debt} class with weights 5.3 and 3.2, respectively, and the feature ``something'' relates to the \textit{without-technical-debt} class with a weight of 4.1. Then, to classify the comment ``this is a dirty hack it's better to do something'' from our test data, all features present in the comment will be examined and the following scores would be calculated: $weight_{design-debt}=8.5$ (i.e., the sum of ``hack'' and ``dirty'' feature weights) and  $weight_{without-technical-debt}=4.1$.
Since $weight_{design-debt}$ is larger than $weight_{without-technical-debt}$, the comment will be classified as design debt.

For each analyzed project, we collect the features used to identify the \SATD comments. These features are provided by the \revised{maximum entropy classifier}{R2-11} as output and stored in a text file. The features are written in the file according to their weights in descending order (starting from more relevant, ending to less relevant features). Based on these files, we rank the words calculating the average ranking position of the analyzed features across the ten different projects. 

\noindent \textbf{Results:} Table \ref{tbl:top_ten_features} shows the top-10 textual features used to identify  \SATD in the ten studied projects, ordered by their average ranking. The first column shows the ranking of each textual feature, the second column lists the features used in the identification of \emph{design} \SATD, and the third column lists the textual features used to identify \emph{requirement} \SATD.

\begin{table}[!thb]
    \begin{center}
        \caption{Top-10 Textual Features Used to Identify Design and Requirement Self-Admitted Technical Debt}
        \label{tbl:top_ten_features}
        \vspace{-3mm}
        \begin{tabular}{l| l l }
        \toprule
        \textbf{Rank} & \textbf{Design Debt} & \textbf{Requirement Debt}  \\
        \midrule
         \textbf{1}  & \textbf{hack}       &   \textbf{todo}     \\
         \textbf{2}  & \textbf{workaround} &   \textbf{needed}   \\
         \textbf{3}  & yuck!      &   \textbf{implementation}    \\
         \textbf{4}  & kludge     &   \textbf{fixme}             \\
         \textbf{5}  & stupidity  &   \textbf{xxx}               \\
         \textbf{6}  & needed?    &   ends?             \\
         \textbf{7}  & columns?   &   convention        \\
         \textbf{8}  & unused?    &   configurable      \\
         \textbf{9}  & wtf?       &   apparently        \\
         \textbf{10} & todo       &   fudging           \\
        \bottomrule
        \end{tabular}
    \end{center}    
\end{table}

From Table~\ref{tbl:top_ten_features} we observe that the top ranked textual features for design \SATD, i.e., \textit{hack}, \textit{workaround}, \textit{yuck!}, \textit{kludge} and \textit{stupidity}, indicate sloppy code, or mediocre source code quality. For example, we have the following comment that was found in JMeter:
\begin{displayquote}
\textit{``\textbf{Hack} to allow entire URL to be provided in host field''}
\end{displayquote}
Other textual features, such as \textit{needed?}, \textit{unused?} and \textit{wtf?} are questioning the usefulness or utility of a specific source code fragment, as indicated by the following comment also found in JMeter:
\begin{displayquote}
\textit{``TODO: - is this \textbf{needed?}''}
\end{displayquote}

\revised{For requirement \SATD, the top ranked features, i.e., \textit{todo}, \textit{needed}, \textit{implementation}, \textit{fixme} and \textit{xxx} indicate the need to complete requirements in the future that are currently partially complete. An indicative example is the following one found in JRuby:
\begin{displayquote}
\textit{``\textbf{TODO:} implement, won't do this now''}
\end{displayquote}
Some of the remaining lower ranked textual features, such as \textit{convention}, \textit{configurable} and \textit{fudging} also indicate potential incomplete requirements, as shown in the following comments:
\begin{displayquote}
\textit{``Need to calculate this... just \textbf{fudging} here for now''} [from JEdit]
\end{displayquote}
\begin{displayquote}
\textit{``could make this \textbf{configurable}''} [from JFreeChart]
\end{displayquote}
\begin{displayquote}
\textit{``TODO: This name of the expression language should be \textbf{configurable} by the user''} [from ArgoUML]
\end{displayquote}
\begin{displayquote}
\textit{``TODO: find a way to check the manifest-file, that is found by naming \textbf{convention}''} [from Apache Ant]
\end{displayquote}
}{R1-11}

It should be noted that the features highlighted in bold in Table~\ref{tbl:top_ten_features} appear in all top-10 lists extracted from each one of the ten training datasets, and therefore can be considered as more \textit{universal} features compared to the others.

We also observe that it is possible for a single textual feature to indicate both design and requirement \SATD. However, in such cases, the ranking of the feature is different for each kind of debt. For example, the word ``todo'' is ranked tenth for design debt, whereas it is ranked first for requirement debt. This finding is intuitive, since requirement debt will naturally be related to the implementation of future functionality.

It is important to note here that although we present only the top-10 textual features, the classification of the comments is based on a combination of a large number of textual features. In fact, two different types of textual features are used to classify the comments, namely positive and negative weight features. Positive weight features will increase the total weight of the vote suggesting that the classification should be equal to the class of the feature (i.e., design or requirement debt). On the other hand, negative weight features will decrease the total weight of the vote suggesting a classification different from the class of the feature. On average, the number of positive weight features used to classify design and requirement debt is 5,014 and 2,195, respectively. The exact number of unique textual features used to detect \SATD for each project is shown in Table \ref{tbl:features_per_project}. The fact that our NLP-based approach leverages so many features helps to explain the significant improvement we are able to achieve over the state-of-the-art, which only uses 62 patterns. In comparison, our approach leverages 35,828 and 34,056 unique textual features for detecting comments with design and requirement debt, respectively.

\begin{table*}[!hbt]
    \begin{center}
        \caption{Number of Unique Textual Features Use to Detect Design and Requirement Debt for Each Project}
        \label{tbl:features_per_project}
        \begin{tabular}{l| c c c|| c c c}
        \toprule
        \multirow{4}{*}{\textbf{\thead{Project}}} & \multicolumn{3}{c||}{\textbf{\thead{Design debt}}} & \multicolumn{3}{c}{\textbf{\thead{Requirement debt}}} 
        \\
        \cmidrule{2-7}
        & \textbf{\thead{Positive\\Weight\\Features}} & \textbf{\thead{Negative\\Weight\\Features}} & \textbf{\thead{\# of\\Features}} & \textbf{\thead{Positive\\Weight\\Features}} & \textbf{\thead{Negative\\Weight\\Features}} & \textbf{\thead{\# of\\Features}}\\
        \midrule
        \textbf{Ant}          & 5,299 & 23,623 & 28,922 & 1,812 & 27,673 & 29,485 \\
        \textbf{ArgoUML}      & 3,917 & 26,012 & 29,929 & 2,779 & 27,260 & 30,039 \\
        \textbf{Columba}      & 5,255 & 24,182 & 29,437 & 2,433 & 27,561 & 29,994 \\
        \textbf{EMF}          & 5,346 & 23,667 & 29,013 & 1,889 & 27,637 & 29,526 \\
        \textbf{Hibernate}    & 4,914 & 24,070 & 28,984 & 2,748 & 26,654 & 29,402 \\
        \textbf{JEdit}        & 5,042 & 24,644 & 29,686 & 1,831 & 28,267 & 30,098 \\
        \textbf{JFreeChart}   & 5,361 & 23,530 & 28,891 & 1,902 & 27,439 & 29,341 \\
        \textbf{Jmeter}       & 5,172 & 23,916 & 29,088 & 1,893 & 27,716 & 29,609 \\
        \textbf{JRuby}        & 4,856 & 24,553 & 29,409 & 2,850 & 27,085 & 29,935 \\
        \textbf{SQuirrel}     & 4,982 & 25,146 & 30,128 & 1,814 & 26,914 & 28,728 \\
        \midrule
        \textbf{Average}       & 5,014 & 24,334 & 29,348 & 2,195  & 27,420 & 29,615 \\      
        \textbf{Total unique}  & 6,327  & 31,518 & 35,828   & 4,015  & 32,954 & 34,056 \\
        \bottomrule
        \end{tabular}
    \end{center}    
\end{table*}

\begin{comment}
\begin{figure*}[!thb]
	\centering
	\subfigure[Ant Design Debt Classification]{\includegraphics[width=0.49\textwidth]{figures/design_ant.pdf}
		\label{fig:design_ant_result}}
        \subfigure[Design Debt Average Percentage of F1-measure]{\includegraphics[width=0.49\textwidth]{figures/design_percentage_f1_measure.pdf}
    \label{fig:design_percentage_f1_measure}}

    
	\subfigure[ArgoUml Requirement Debt Classification]{\includegraphics[width=0.49\textwidth]{figures/implementation_argo.pdf}
		\label{fig:implementation_argo_result}}
        \subfigure[Requirement Debt]{\includegraphics[width=0.49\textwidth]{figures/requirement_percentage_f1_measure.pdf}
    \label{fig:requirement_percentage_f1_measure}}
	\caption{F1-measure Achieved per Iteration }
\end{figure*}
\end{comment}


\conclusionbox{We find that design and requirement debt have their own textual features that best indicate such \SATD comments. For design debt, the top textual features indicate sloppy code or mediocre code quality, whereas for requirement debt they indicate the need to complete a partially implemented requirement in the future.}

\vspace{3mm}
\noindent\rqiii
\vspace{3mm}

\noindent \textbf{Motivation:} Thus far, we have shown that our NLP-based approach can effectively identify comments expressing \SATD. However, we conjecture that the performance of the classification depends on the amount of training data. At the same time, creating the training dataset is a time consuming and labor intensive task. So, the question that arises is: how much training data do we need to effectively classify the source code comments? If we need a very large number of comments to create our training dataset, our approach will be more difficult to extend and apply for other projects. On the other hand, if a small dataset can be used to reliably identify comments with \SATD, then this approach can be applied with a minimal effort, i.e., less training data. That said, intuitively we expect that the performance of the \revised{maximum entropy classifier}{R2-11} will improve as more comments are being added to the training dataset.

\noindent \textbf{Approach:} To answer this research question, \revised{we follow a systematic process where we incrementally add training data and evaluate the performance of the classification.
More specifically, we shuffle the comments from all projects into a big dataset. Then, we split this dataset into 10 equally-sized parts making sure that each part has the same ratio of comments with \SATD and without technical debt. Next, we use one of the ten parts to test the max entropy classifier and the other 9 to train it. The training data is incrementally fed into the classifier by batches of 100 comments each time, and we also make sure that each batch contains the same ratio of comments with \SATD and without technical debt. We repeat this process for each one of the 10 parts and report our findings.

To determine how much training data is required to effectively identify \SATD comments, we compute the F1-measure values after each iteration (i.e., the addition of a batch of 100 comments) and record the iteration that achieves the highest F1-measure.
Then we find the iterations in which at least 80\% and 90\% of the maximum F1-measure value is achieved, and report the number of comments added up to those iterations.
%However, instead of recording the number of projects that were used to train the dataset we report the number of comments that was used to achieve this F1-measure. The number of comments is computed in multiples of 100, due to the size of each batch added to the training dataset.	
}{R2-14}
\begin{comment}
To execute the first experiment, we use one project as testing data, and the remaining nine projects to train, since we have 10 projects in our dataset.}{R2-14} However, we do not train the \revised{maximum entropy classifier}{R2-11} with all nine projects, instead, we add each project incrementally. We repeated this process for each project and report on our findings.

\revised{It is important to notice that the order we added each project was not aleatory. After some experimentation we decided that the best way to train the max entropy classifier is to add first to the training dataset projects with more of the \SATD comments being identified. This way we could achieve better classification performance quicker. Furthermore, adding projects in a randomly fashion would impact the classification performance and prevent us to properly analyze the results of the first experiment.}{R2-15}

To determine how much data is required to effectively identify \SATD comments, we compute the F1-measure after each iteration (an iteration is simply a run with a different size of training data). We record the iteration that achieves the highest F1-measure and the number of projects used in the training dataset to achieve this F1-measure. Then, we record the number of projects needed to achieve at least 90\% and 80\% of the maximum F1-measure.

For example, if the maximum F1-measure is 0.85 and it is achieved in the eighth iteration (i.e., using 8 projects in the training dataset), and during the fourth iteration we achieve a F1-measure of 0.80, then we say that we can achieve at least 90\% (94\% to be exact) of the maximum F1-measure with a training dataset constructed from just 4 projects. Since the results will differ for the different projects, we repeat this analysis for all projects and present the average F1-measures across all projects.



\noindent \textbf{Per-project based experiment results - design debt:} Figure~\ref{fig:design_ant_result} shows the F1-measure using different sizes of training data for the Ant project. Due to space, we discuss the results for a representative project (Ant) in this section, however, \revised{Figure~\ref{fig:design_percentage_f1_measure} presents the maximum percentage average achieved for all projects in each one of the analyzed iterations.}{R3-9}. From Figure~\ref{fig:design_ant_result}, we find that the maximum F1-measure improves as we increase the number of projects (i.e., iterations), achieving the highest F1-measure in the seventh iteration and slightly decreasing afterwards. The horizontal lines in the figure show the 80\% and 90\% of the highest F1-measure. We can see from Figure~\ref{fig:design_ant_result} that with 1,499 comments (i.e., from 3 projects) and 1,815 comments (i.e., from 4 projects), we can achieve 80\% and 90\% of the highest F1-measure, respectively. This amounts to a reduction of  37.6\% and 24.5\% in training data to achieve 80\% and 90\% of the maximum F1-measure, respectively. Considering the tradeoff in accuracy versus the amount of training data, for Ant, using only 3 or 4 projects provides the best tradeoff.

We analyzed all iterations from all projects to determine the iterations that achieve the best F1-measure performance. To measure that, we calculate the average percentage of the maximum F1-measure for each iteration\revised{as shown in Figure~\ref{fig:design_percentage_f1_measure}}{R3-9}. For example, we take the average percentage of the maximum F1-measure achieved during the first iteration for all projects, then we calculate the same value for all second iterations and so on. We find that, the best performance is achieved during the eighth iteration, with an average maximum F1-measure of 96.57\% using (on average) 2,353 comments to create the training dataset. In comparison, the ninth iteration has an average  maximum F1-measure of 95.99\%, which is slightly lower than the average obtained in the eighth iteration, and uses more comments in the training dataset (i.e., 2,432).

Table \ref{tbl:design_iteration_performance} shows the average percentage of the maximum F1-measure for each iteration. The first column shows the iteration number. The second column shows the average percentage of the maximum F1-measure achieved for each iteration. The third column presents the delta of the average percentage of the maximum F1-measure between one iteration and the previous one. The fourth column shows the average number of comments used in the training dataset of that specific iteration. From Table~\ref{tbl:design_iteration_performance} we observe that, on average, we can achieve more than 80\% and 90\% of the maximum F1-measure in the second and third iterations, respectively. To achieve more than 80\% and 90\% of the maximum F1-measure, we require 1,106 (49.13\% of total comments required in seventh iteration) and 1,444 (64.14\% of total comments required in seventh iteration) comments, respectively. Moreover, we see that the second and third iterations provide the highest delta between iterations.
\end{comment}
\begin{figure*}[!thb]
    \centering
    \subfigure[Design Debt]{\includegraphics[width=0.49\textwidth]{figures/appendix/ten_fold_validation_design/ten_fold_validation_average_100.pdf}
    \label{fig:design_debt_per_comment_average}}
    \subfigure[Requirement Debt]{\includegraphics[width=0.49\textwidth]{figures/appendix/ten_fold_validation_requirement/ten_fold_validation_average_100.pdf}
    \label{fig:requirement_debt_per_comment_average}}
	\vspace{-2mm}
    \caption{F1-measure achieved by incrementally adding batches of 100 comments in the training dataset.}
\end{figure*}


\revised{\noindent \textbf{Results - design debt:} Figure~\ref{fig:design_debt_per_comment_average} shows the average F1-measure values obtained when detecting design \SATD, while adding batches of 100 comments. We find that the F1-measure score improves as we increase the number of comments in the training dataset, and the highest value (i.e., 0.824) is achieved with 42,700 comments. However, the steepest improvement in the F1-measure performance takes place within the first 2K-4K comments. Additionally, 80\% and 90\% of the maximum F1-measure value is achieved with 3,900 and 9,700 comments in the training dataset, respectively.
Since each batch of comments consists of approximately 5\% (i.e., $\frac{2,703}{58,122}$) comments with design \SATD, the iteration achieving 80\% of the maximum F1-measure value contains 195 comments with design \SATD,
while the iteration achieving 90\% of the maximum F1-measure value contains 485 such comments.
In conclusion, to achieve 80\% of the maximum F1-measure value, we need only 9.1\% (i.e., $\frac{3,900}{42,700}$) of the training data, while to achieve 90\% of the maximum F1-measure value, we need only 22.7\% (i.e., $\frac{9,700}{42,700}$) of the training data.
}{R2-14}

\begin{comment}
\revised{\noindent \textbf{Per-project based experiment results - requirement debt:}}{R2-14} We find that, although there is a variation in the F1-measure value during the first 3 iterations, they are not as preeminent as the variation found in design \SATD analysis. The F1-measure in requirement \SATD tends to be more constant through the iterations, and the first iteration has a high percentage of the maximum F1-measure achieved for each project. This shows that the way developers indicate requirement debt does not vary between different application domains as much as in design debt. This uniformity in requirement \SATD comments allows a good classification even with a small number of comments in the training dataset. We elaborate more on this point later in Section~\ref{sec:discussion}.

Figure \ref{fig:implementation_argo_result} shows the F1-measure for different iterations in ArgoUML. The highest F1-measure of 0.65 is achieved in the third iteration. From Figure~\ref{fig:implementation_argo_result}, we observe that we achieve more than 80\% of the highest F1-measure in the first iteration and more than 90\% in the second and subsequents iterations. The reduction in comments is 28.64\% and 62.31\% for the 90\% and 80\% of the maximum F1-measure, respectively.

Table \ref{tbl:requirement_iteration_performance} shows the average percentage of the maximum F1-measure for each iteration. Unlike the case of design debt, for requirement debt, the best F1-measure is achieved in the first iteration. This shows that using as few as 380 comments, we can effectively detect requirement \SATD\revised{as we can also see from Figure~\ref{fig:requirement_percentage_f1_measure}.}{R3-9}.

\begin{table}[!thb]
    \begin{center}
        \caption{Average Maximum F1-measure for Design Debt for All Projects}
        \label{tbl:design_iteration_performance}
        \begin{tabular}{l| c c c}
        \toprule
        \textbf{\thead{Iteration\\Number}} & \textbf{\thead{Average\%\\of maximum\\F1-measure}} & \textbf{\thead{$\Delta$\\between\\iterations}} & \textbf{\thead{Average\\comments}} \\ 
        \midrule
         \textbf{1}  &  0.718 &  -      & 756   \\  
         \textbf{2}  &  0.856 & 0.138   & 1,106 \\  
         \textbf{3}  &  0.924 & 0.068   & 1,444 \\  
         \textbf{4}  &  0.912 & -0.012  & 1,717 \\  
         \textbf{5}  &  0.927 & 0.016   & 1,919 \\  
         \textbf{6}  &  0.930 & 0.002   & 2,108 \\  
         \textbf{7}  &  0.963 & 0.029   & 2,251 \\  
         \textbf{8}  &  0.965 & 0.006   & 2,353 \\  
         \textbf{9}  &  0.959 & -0.002  & 2,432 \\  
        \bottomrule
        \end{tabular}
    \end{center}    
\end{table}

\begin{table}[!thb]
	\begin{center}
		\caption{Average Maximum F1-measure for Requirement Debt for All Projects}
		\label{tbl:requirement_iteration_performance}
		\begin{tabular}{l| c c c}
			\toprule
			\textbf{\thead{Iteration\\Number}} & \textbf{\thead{Average\%\\of maximum\\F1-measure}} & \textbf{\thead{$\Delta$\\between\\iterations}} & \textbf{\thead{Average\\comments}} \\
			\midrule
			\textbf{1}  &  0.873 &   -      &  380  \\  
	        \textbf{2}  &  0.772 & -0.101   &  481  \\
	        \textbf{3}  &  0.778 & 0.006    &  541  \\  
	        \textbf{4}  &  0.806 & 0.028    &  588  \\
	        \textbf{5}  &  0.795 & -0.011   &  620  \\
	        \textbf{6}  &  0.819 & 0.024    &  638  \\
			\textbf{7}  &  0.837 & 0.018    &  654  \\  
	        \textbf{8}  &  0.833 & -0.004   &  668  \\  
	        \textbf{9}  &  0.805 & -0.028   &  681  \\  
			\bottomrule
		\end{tabular}
	\end{center}    
\end{table}
\end{comment}

\revised{\noindent \textbf{Results - requirement debt:} Figure~\ref{fig:requirement_debt_per_comment_average} shows the average F1-measure values obtained when detecting requirement \SATD, while adding batches of 100 comments. As expected, the F1-measure increases as we add more comments into the training dataset, and again the steepest improvement takes place within the first 2-3K comments.
	The highest F1-measure value (i.e., 0.753) is achieved using 51,300 comments of which 675 are requirement \SATD.
	Additionally, 80\% of the maximum F1-measure score is achieved with 2,600 comments, while 90\% of the maximum F1-measure score with 11,800 comments in the training dataset.
	Each batch contains two comments with requirement \SATD, since the percentage of such comments is 1.3\% (i.e., $\frac{757}{58,122}$) in the entire dataset.
	As a result, the iteration achieving 80\% of the maximum F1-measure value contains 52 comments with requirement \SATD,
	while the iteration achieving 90\% of the maximum F1-measure value contains 236 such comments.
	In conclusion, to achieve 80\% of the maximum F1-measure value, we need only 5\% (i.e., $\frac{2,600}{51,300}$) of the training data, while to achieve 90\% of the maximum F1-measure value, we need only 23\% (i.e., $\frac{11,800}{51,300}$) of the training data.
	}{R2-14}

\conclusionbox{We find that to achieve a performance equivalent to 90\% of the maximum F1-measure score, only 23\% of the comments are required for both design and requirement \SATD.
	For a performance equivalent to 80\% of the maximum F1-measure score, only 9\% and 5\% of the comments are required for design and requirement \SATD, respectively.}