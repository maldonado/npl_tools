% -*- root: main.tex -*-
% \revised{\noindent\textbf{Construct validity and reliability}}{R1-13} considers the relationship between theory and observation, in case the measured variables do not measure the actual factors. The training dataset used by us heavily relied on a manual analysis and classification of the code comments from the studied projects.\revised{ Like any human activity, our manual classification is subject to personal bias. To reduce this bias, we took a statistically significant sample of our classified comments and asked a Master's student, who is not an author of the paper, to manually classify them. Then, we calculate the Kappa's level of agreement between the two classifications. The level of agreement obtained was +0.81, which according to Fleiss~\cite{Fleiss1981measurement} is characterized as an excellent inter-rater agreement (values larger than +0.75 are considered excellent).}{R1-5} \revised{Nevertheless, due to the irregular data distribution of our significant sample (which has many more comments without technical debt, than comments with the other classes of debt), we also measured Kappa's level of agreement for design and requirement \SATD separately. The level of agreement obtained for design and requirement \SATD was +0.75 and +0.84, respectively.}{R2-10}  

\noindent\textbf{Construct validity and reliability} considers the relationship between theory and observation, in case the measured variables do not measure the actual factors. The training dataset used by us heavily relied on a manual analysis and classification of the code comments from the studied projects. Like any human activity, our manual classification is subject to personal bias. To reduce this bias, we took a sample of our classified comments and asked a Phd's candidate, who is not an author of the paper, to manually classify them. Then, we calculate the Kappa's level of agreement between the two classifications. The level of agreement obtained was +0.84, which according to Fleiss~\cite{Fleiss1981measurement} is characterized as an excellent inter-rater agreement (values larger than +0.75 are considered excellent). Nevertheless, we also measured Kappa's level of agreement for design and requirement \SATD separately. The level of agreement obtained for design and requirement \SATD was +0.89 and +0.83, respectively.

When performing our study, we used well-commented Java projects. Since our approach heavily depends on code comments, our results and performance measures may be impacted by the quantity and quality of comments in a software project. 

\revised{Considering the intentional misrepresentation of measures, it is possible that even a well commented project does not contain self-admitted technical debt. Given the fact that the developers may opt to not express themselves in source code comments. In our study, we made sure that we choose case studies that are appropriately commented for our analysis.}{R1-14}   

\revised{On the same point, using comments to determine some self-admitted technical debt may not be fully representative, since comments or code may not be updated consistently. However, previous work shows that changes in the source code are consistent to changes in comments~\cite{Fluri2007WCRE,Potdar2014ICSME}. In addition, it is possible that a variety of technical debt that is not self-admitted is present in the analyzed projects. However, since the focus of this paper is to improve the detection of the most common types of \SATD, considering all technical debt is out of the scope of this paper.}{R3-2}

\revised{Lastly, our approach depends on the correctness of the underlying tools we use. To mitigate this risk, we used tools that are commonly used by practitioners and by the research community, such as JDeodorant for the extraction of source code comments and for investigating the overlap with code smells (Section~\ref{sec:overlap_with_static_analysis_tools}) and the Stanford Classifier for training and testing the max entropy classifier used in our approach.}{R1-14}

\noindent \textbf{External validity} considers the generalization of our findings. All of our findings were derived from comments in open source projects. \revised{To minimize the threat to external validity, we chose open source projects from different domains. That said, our results may not generalize to other open source or commercial projects, projects written in different languages, projects from different domains and/or technology stacks. In particular, our results may not generalize to projects that have a low number or no comments or comments that are written in a language other than English.}{R1-4} 