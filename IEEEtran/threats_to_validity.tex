% -*- root: main.tex -*-
\revised{\noindent\textbf{Construct validity and reliability}}{R1-13} considers the relationship between theory and observation, in case the measured variables do not measure the actual factors. The training dataset used by us heavily relied on manual analysis of the code comments from the studied projects.\revised{ Like any human activity, our manual classification is subject to personal bias. To reduce this bias, we took a statistically significant sample of our classified comments and asked a masters student, who is not an author of the paper, to manually classify them.Then, we calculate the kappa's level of agreement between the classification given by the two different students. The level of agreement obtained was 0.81, which according to Fleiss~\cite{Fleiss1981measurement} is characterized as excellent inter-rater agreement.}{R1-5}   

When performing our study, we used well-commented Java projects. Since our approach heavily depends on code comments, our results and performance measures may be impacted by the quantity and quality of comments in a software project. 

\revised{
Considering the intentional misrepresentation of measures it is possible that even a well commented project does not contain self-admitted technical debt. Given the fact that the developers may opt to not express themselves in source code comments. In our study, we made sure that we chose case studies that were well commented for our analysis.   

Lastly, our approach depends on the correctness of the underlying tools we use. To mitigate this risk, we used tools that are commonly used by practitioners and by the research community such as JDeodorant and the Stanford Classifier.}{R1-14}

%\revised{Although we made an effort to analyze projects from different application domains we did not cover all application domains available to analysis. Therefore, it is possible that different application domains may demand more specific training data (i.e, manual classified comments) to achieve similar results as our study reports. 

%Moreover, it is still an open question if the current manual classified dataset can be applied effectively on other software projects created using different languages or technology stacks.} 


\noindent \textbf{External validity} considers the generalization of our findings. All of our findings were derived from comments in open source projects. \revised{To minimize the threat to external validity, we chose open source projects from different domains. That said, our results may not generalize to other open source or commercial projects, projects written in different languages, projects from different domains and/or technology stacks. In particular, our results may not generalize to projects that have a low number or no comments.}{R1-4} 

% Also, our approach is based on the Stanford NLP classifier. Hence, the accuracy of our approach is affected by the accuracy of the Stanford NLP tool. 