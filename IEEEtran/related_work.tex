Our work uses code comments to detect \SATD through the use of a NLP technique. Therefore, we divide the related work into three categories: source code comments, technical debt and Natural Language Processing in Software Engineering.

\subsection{Source Code Comments}

A number of studies examined the co-evolution of source code comments and the rationale for changing code comments. For example, Fluri \textit{et al.}~\cite{Fluri2007WCRE} analyzed the co-evolution of source code and code comments, and found that 97\% of the comment changes are consistent. Tan \textit{et al.}~\cite{Tan2012ICST} proposed a novel approach to identify inconsistencies between Javadoc comments and method signatures. Malik \textit{et al.} \cite{Malik2008ICSM} studied the likelihood of a comment to be updated and found that call dependencies, control statements, the age of the function containing the comment, and the number of co-changed dependent functions are the most important factors to predict comment updates.

Other works used code comments to understand developer tasks. For example. Storey \textit{et al.}~\cite{Storey2008ICSE} analyzed how task annotations (e.g., TODO, FIXME) play a role in improving team articulation and communication. The work closest to ours is the work by Potdar and Shihab~\cite{Potdar2014ICSME}, where code comments were used to identify technical debt. 

Similar to some of the prior work, we also use source code comments to identify technical debt. However, our main focus is on the detection of different types of \SATD. As we have shown, our approach yields different and better results in detection of \SATD.
%Furthermore, we propose an approach to identify \SATD, that is derived from source code comments and natural language processing techniques, to detect \SATD.

\subsection{Technical Debt}

A number of studies have focused on detection and management of technical debt. For example, Seaman \textit{et al.}~\cite{Seaman2011}, Kruchten \textit{et al.}~\cite{Kruchten2013IWMTD} and Brown \textit{et al.}~\cite{Brown2010MTD} make several reflections about the term technical debt and how it has been used to communicate the issues that developers find in the code in a way that managers can understand. 

Other work focused on the detection of technical debt. Zazworka \textit{et al.} \cite{Zazworka2013CSE} conducted an experiment to compare the efficiency of automated tools in comparison with human elicitation regarding the detection of technical debt. They found that there is a small overlap between the two approaches, and thus it is better to combine them than replace one with the other. In addition, they concluded that automated tools are more efficient in finding defect debt, whereas developers can realize more abstract categories of technical debt.

In a follow up work, Zazworka \textit{et al.}~\cite{Zazworka2011MTD} conducted a study to measure the impact of technical debt on software quality. They focused on a particular kind of design debt, namely, God Classes. They found that God Classes are more likely to change, and therefore, have a higher impact on software quality. Fontana \textit{et al.}~\cite{Fontana2012MTD} investigated design technical debt appearing in the form of code smells. They used metrics to find three different code smells, namely God Classes, Data Classes and Duplicated Code. They proposed an approach to classify which one of the different code smells should be addressed first, based on its risk. Ernst \textit{et al.} ~\cite{Ernst2015FSE} conducted a survey with 1,831 participants and found that architectural decisions are the most important source of technical debt.

Our work is different from the work that uses code smells to detect design technical debt since we use code comments to detect technical debt. Moreover, our approach does not rely on code metrics and thresholds to identify technical debt and can be used to identify bad quality code symptoms other than bad smells.

More recently, Potdar and Shihab~\cite{Potdar2014ICSME} extracted the comments of four projects and analyzed more than 101,762 comments to come up with 62  patterns that indicates self-admitted technical debt. Their findings shows that 2.4\% - 31\% of the files in a project contain self-admitted technical debt. Our earlier work ~\cite{Maldonado2015MTD} examined more than 33 thousands comments to classify the different types of \SATD found in source code comments. Farias \textit{et al.} ~\cite{Farias2015MTD} proposed a contextualized vocabulary model for identifying TD on comments that uses word classes and code tags in the process. 

Our work also uses code comments to detect design technical debt. However, we use these code comments to train a NLP Classifier to automatically identify technical debt. Also, our focus is on \emph{self-admitted} design and requirement technical debt.

\subsection{NLP in Software Engineering}

A number of studies leveraged NLP in software engineering, mainly for the traceability of requirements, program comprehension and software maintenance. For example, Lormans and van Deursen~\cite{Lormans2006CSRM} used latent semantic indexing (LSI) to create traceable links between requirements and test cases and requirements to design implementations. Hayes \textit{et al.}~\cite{Hayes2005, Hayes2006TSE} created a tool called RETRO that applies information retrieval techniques to trace and map requirements to designs. Yadla \textit{et al.}~\cite{yadla2005tracing} further enhanced the RETRO tool and linked requirements to issue reports. On the other hand, Runeson \textit{et al.}~\cite{Runeson2007ICSE} implemented a NLP-based tool to automatically identify duplicated issues reports, they found that 2/3 of the possible duplicates examined in their study can be found with their tool. Canfora and Cerulo \cite{Canfora2005ISSM} link a change request with the correspondent set of source files using NLP techniques. The performance of the approach is evaluated on four open source projects.  

The prior work motivated us to use NLP techniques. However, our work is different from the aforementioned since we apply NLP techniques on code comments to identify \SATD, rather than use it for traceability.