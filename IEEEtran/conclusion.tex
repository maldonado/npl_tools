Technical debt is a term being used to express none optimal solutions, such as hacks and workarounds, that are applied during the software development process. Although these none optimal solutions can help achieve immediate pressing goals, most often they will have a negative impact on the project maintainability~\cite{Zazworka2011MTD}. 

Our work focuses on the identification of \SATD through the use of Natural Language Processing. We analyzed the comments of 10 open source projects namely Ant, ArgoUML, Columba, EMF, Hibernate, JEdit, JFreeChart, Jmeter, JRuby and SQuirrel SQL. These projects are considered well commented and they belong to different application domains. The comments of these projects were manually classified into specific types of technical debt such as design, requirement, defect, documentation and test debt. Next, we selected 61,664 comments from this dataset (i.e., those classified as design \SATD, requirement \SATD and without technical debt) to train the NLP Classifier, and then this tool was used to identify  design and requirement \SATD automatically.
% \nikos{I think we didn't give the entire dataset as input, but only the design/requirement debt comments and no-debt comments}

We first evaluated the performance of our approach by comparing our F1 measure with two other baseline F1 measures, i.e., the comment patterns baseline and the simple (random) baseline. We shown that our approach outperforms the state-of-the-art on average 4.5 times in the identification of design \SATD. Moreover, our approach can identify requirement \SATD, deed that the state-of-the-art fails to achieve. Our approach performance surpasses the simple (random) baseline on average 7.1 and 18 times for design and requirement \SATD, respectively. Then, we explored the characteristics of the features (i.e., words) used to classify \SATD. We find that the words used to express design and requirement \SATD are different from each other. The three strongest indicators of design \SATD are `hack', `workaround' and `yuck!', whereas, `todo', `needed' and `implementation' are the strongest indicators of requirement debt. In addition, we find that even using a low number of \SATD comments in the training dataset can achieve high classification performance. In fact, our results show that developers use a richer vocabulary to express design \SATD and a training dataset of at least 1,444 design \SATD comments is necessary to obtain a satisfactory classification. On the other hand, requirement \SATD is expressed in a more uniform way, and with a training dataset of 380 \SATD comments it is possible to classify with success requirement \SATD automatically.

In the future, we believe that more analysis is needed to fine tune the use of the current training dataset in order to achieve maximum efficiency in the prediction of \SATD comments. For example, using subsets of our training dataset can be more suitable for some applications than using the whole dataset due to domain particularities. However, the results thus far are not to be neglected as our approach has the best F1-measure performance on every analyzed project. Moreover, to enable future research, we make the dataset created in this study publicly available. We believe that it will be a good starting point for researchers interested in identifying technical debt through comments and even using different Natural Language Processing techniques on it. Lastly, we plan to use the findings of this study to build a tool that will support software engineers in the task of identifying and managing \SATD. 


% \emad{we have 3 results there....we really need to simply this and have 1 or max 2 results.}